{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "20607127",
   "metadata": {},
   "source": [
    "# Thought Cascade: Building and Evaluating Agentic AI Systems with Small Language Models\n",
    "\n",
    "*An Interactive Journey into Iterative Intelligence*\n",
    "\n",
    "## Submission Requirements\n",
    "\n",
    "1. **Complete all coding implementations** marked with `TODO` in this notebook (for example, Questions 1, 4, 5, and 6).\n",
    "2. **Complete all analysis sections** explicitly labeled \"Required Analysis\".\n",
    "3. **Run all cells** so that key tables and plots (e.g., single-shot metrics, STRICT tests, QuixBugs results, and ReAct loop outcomes) are visible in your saved notebook.\n",
    "4. **Follow course submission instructions** for exporting your answers (avoid submitting unnecessarily long full-notebook PDFs).\n",
    "\n",
    "## Introduction: The Rise of Agentic AI\n",
    "\n",
    "Consider how humans solve complex problems. We rarely get the answer right on the first try. Instead, we form a hypothesis, test it, observe what happens, and refine our approach. We repeat this cycle until we succeed.\n",
    "\n",
    "Traditional single-shot prompting forces models to \"think\" once and commit to an answer immediately. This is like asking a student to solve a complex math problem without scratch paper or the ability to check their work. Agentic AI, by contrast, gives models the ability to work through problems systematically.\n",
    "\n",
    "### The Agentic Loop\n",
    "\n",
    "At its core, an agentic system operates in a loop of **Reasoning**, **Acting**, and **Observing**.\n",
    "\n",
    "![The ReAct Loop: Reasoning, Acting, and Observing](images/react_diagram.png)\n",
    "*Figure 1: The ReAct Loop (Source: [Yao et al., 2022](https://react-lm.github.io/))*\n",
    "\n",
    "**What is Agentic AI?**\n",
    "\n",
    "Agentic AI represents a fundamental shift in how we interact with language models. Instead of treating models as passive question-answering systems, we treat them as active problem solvers. These systems can reason about problems iteratively, act in their environment using tools, observe the results, and adapt their approach based on feedback.\n",
    "\n",
    "The key insight is that **intelligence emerges not from model size alone, but from the ability to iterate and self-correct**.\n",
    "\n",
    "**The ReAct Framework**\n",
    "\n",
    "In this notebook, we'll focus on **ReAct** (Reasoning and Acting), one of the most successful patterns for building agentic systems. ReAct interleaves reasoning traces with actions, creating a feedback loop that enables self-improvement. The principles we'll learn—decomposition, tool use, memory, and self-evaluation—apply broadly to any agentic system.\n",
    "\n",
    "## Learning Objectives\n",
    "\n",
    "By completing this notebook, you will:\n",
    "\n",
    "1. **Understand** the theoretical foundations of agentic AI and why iteration beats size\n",
    "2. **Build** complete agentic systems using the ReAct pattern\n",
    "3. **Measure** system performance at the component level, not just end-to-end\n",
    "4. **Design** tools that enable effective agent-environment interaction\n",
    "5. **Compare** different models and understand their trade-offs in agentic contexts\n",
    "6. **Diagnose** failure modes and implement recovery strategies\n",
    "7. **Apply** agentic patterns to diverse problem domains"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e17ec402",
   "metadata": {},
   "source": [
    "## Section 0: Environment Setup and Model Selection\n",
    "\n",
    "### Understanding Local Model Inference\n",
    "\n",
    "We'll use **Hugging Face Transformers** for running models locally. This approach has several advantages:\n",
    "- **Privacy**: Your data never leaves your machine\n",
    "- **Cost**: No API fees or rate limits\n",
    "- **Control**: Full control over model versions and configurations\n",
    "- **Learning**: Better understanding of model behavior and limitations\n",
    "\n",
    "The models we'll use are instruction-tuned versions that can run on consumer hardware (GPU recommended but CPU works)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a710f32",
   "metadata": {
    "lines_to_next_cell": 1
   },
   "outputs": [],
   "source": [
    "# Install required packages (uncomment if needed)\n",
    "%pip install transformers torch pandas matplotlib seaborn numpy accelerate\n",
    "\n",
    "import json\n",
    "import time\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import requests\n",
    "import os\n",
    "from typing import Dict, List, Optional, Tuple, Any\n",
    "from dataclasses import dataclass\n",
    "import re\n",
    "import traceback\n",
    "from collections import Counter\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Import transformers\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "import torch\n",
    "\n",
    "# Set style for better visualizations\n",
    "plt.style.use('seaborn-v0_8-darkgrid')\n",
    "sns.set_palette(\"husl\")\n",
    "\n",
    "print(\"Packages imported successfully\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7d349b97",
   "metadata": {},
   "source": [
    "### Hugging Face Model Wrapper\n",
    "\n",
    "We create a wrapper class that mimics the Ollama API but uses Hugging Face Transformers.\n",
    "This allows us to use the same interface throughout the notebook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "47c932f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "class HFModelWrapper:\n",
    "    \"\"\"Wrapper to use Hugging Face models with Ollama-like interface.\"\"\"\n",
    "    \n",
    "    def __init__(self):\n",
    "        self.models = {}        # Cache loaded models by alias (e.g., 'gemma2:2b')\n",
    "        self.tokenizers = {}    # Keep matching tokenizers alongside models\n",
    "        self.device = (\n",
    "            \"mps\" if hasattr(torch.backends, \"mps\") and torch.backends.mps.is_available()\n",
    "            else \"cuda\" if torch.cuda.is_available()\n",
    "            else \"cpu\"\n",
    "        )  # Prefer GPU when available\n",
    "        print(f\"Using device: {self.device}\")\n",
    "        \n",
    "    def pull(self, model_name: str):\n",
    "        \"\"\"Load a model (mimics ollama.pull).\"\"\"\n",
    "        # Map ollama model names to HuggingFace model IDs\n",
    "        model_mapping = {\n",
    "            \"gemma2:2b\": \"google/gemma-2-2b-it\",\n",
    "            \"qwen2.5:3b\": \"Qwen/Qwen2.5-3B-Instruct\",\n",
    "            \"llama3.2:3b\": \"meta-llama/Llama-3.2-3B-Instruct\",\n",
    "            \"smollm:360m\": \"HuggingFaceTB/SmolLM-360M-Instruct\"\n",
    "        }\n",
    "        # TODO: Students - Update these paths if you have models downloaded locally from Project 2\n",
    "        # If these paths are not found, the code will automatically download from HuggingFace.\n",
    "        # You can also set these to None to force a download.\n",
    "        local_model_dirs = {\n",
    "            \"gemma2:2b\": os.path.abspath(\n",
    "                os.path.join(\"..\", \"Project 2 Neural Archaeology (instructor)\", \"models\", \"gemma-2-2b-it\")\n",
    "            ),\n",
    "            \"smollm:360m\": os.path.abspath(\n",
    "                os.path.join(\"..\", \"Project 2 Neural Archaeology (instructor)\", \"models\", \"SmolLM-360M-Instruct\")\n",
    "            )\n",
    "        }\n",
    "        \n",
    "        if model_name not in model_mapping:\n",
    "            raise ValueError(f\"Unknown model: {model_name}\")\n",
    "            \n",
    "        hf_model_id = model_mapping[model_name]\n",
    "        local_dir = local_model_dirs.get(model_name)\n",
    "        if local_dir and os.path.isdir(local_dir):\n",
    "            print(f\"Loading {model_name} from local directory ({local_dir})...\")\n",
    "            hf_model_id = local_dir\n",
    "        else:\n",
    "            print(f\"Loading {model_name} from HuggingFace ({hf_model_id})...\")\n",
    "        \n",
    "        try:\n",
    "            # Load tokenizer\n",
    "            self.tokenizers[model_name] = AutoTokenizer.from_pretrained(hf_model_id)\n",
    "            \n",
    "            # Load model with appropriate settings\n",
    "            self.models[model_name] = AutoModelForCausalLM.from_pretrained(\n",
    "                hf_model_id,\n",
    "                torch_dtype=torch.float16 if self.device != \"cpu\" else torch.float32,\n",
    "                device_map=\"auto\" if self.device == \"cuda\" else None,\n",
    "                low_cpu_mem_usage=True\n",
    "            )\n",
    "            \n",
    "            if self.device != \"cuda\":\n",
    "                self.models[model_name] = self.models[model_name].to(self.device)\n",
    "                \n",
    "            print(f\"  ✓ Model {model_name} loaded successfully\")\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"  ✗ Error loading {model_name}: {e}\")\n",
    "            raise\n",
    "    \n",
    "    def chat(self, model: str, messages: List[Dict[str, str]], **kwargs) -> Dict:\n",
    "        \"\"\"Generate response (mimics ollama.chat).\"\"\"\n",
    "        if model not in self.models:\n",
    "            raise ValueError(f\"Model {model} not loaded. Call pull() first.\")\n",
    "        \n",
    "        # Format messages into a single prompt string for generate()\n",
    "        prompt = self._format_messages(messages)\n",
    "        \n",
    "        # Tokenize\n",
    "        tokenizer = self.tokenizers[model]\n",
    "        inputs = tokenizer(prompt, return_tensors=\"pt\").to(self.device)\n",
    "        \n",
    "        # Generate\n",
    "        with torch.no_grad():\n",
    "            outputs = self.models[model].generate(\n",
    "                **inputs,\n",
    "                max_new_tokens=kwargs.get('max_tokens', 512),\n",
    "                temperature=kwargs.get('temperature', 0.7),\n",
    "                do_sample=True,\n",
    "                pad_token_id=tokenizer.eos_token_id\n",
    "            )\n",
    "        \n",
    "        # Decode\n",
    "        response_text = tokenizer.decode(outputs[0][inputs['input_ids'].shape[1]:], skip_special_tokens=True)\n",
    "        \n",
    "        # Return in Ollama format\n",
    "        return {\n",
    "            'message': {\n",
    "                'content': response_text.strip()\n",
    "            }\n",
    "        }\n",
    "    \n",
    "    def _format_messages(self, messages: List[Dict[str, str]]) -> str:\n",
    "        \"\"\"Format messages into a prompt string.\"\"\"\n",
    "        prompt = \"\"\n",
    "        for msg in messages:\n",
    "            role = msg['role']\n",
    "            content = msg['content']\n",
    "            if role == 'user':\n",
    "                prompt += f\"User: {content}\\n\\n\"\n",
    "            elif role == 'assistant':\n",
    "                prompt += f\"Assistant: {content}\\n\\n\"\n",
    "            elif role == 'system':\n",
    "                prompt += f\"System: {content}\\n\\n\"\n",
    "        \n",
    "        prompt += \"Assistant: \"\n",
    "        return prompt\n",
    "\n",
    "# Create global instance\n",
    "ollama = HFModelWrapper()\n",
    "print(\"HuggingFace model wrapper initialized\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4096e741",
   "metadata": {},
   "source": [
    "### Model Selection: Understanding Trade-offs\n",
    "\n",
    "A compact comparison table for quick reference during the exercises.\n",
    "\n",
    "| Model     | Params | Provider         | Architecture                             | Strengths                             | Trade-offs                         |\n",
    "|-----------|--------|------------------|------------------------------------------|---------------------------------------|------------------------------------|\n",
    "| Gemma 2   | 2B     | Google DeepMind  | Transformer + RoPE                       | Reasoning; instruction following      | Verbose; slower at structured output |\n",
    "| Qwen 2.5  | 3B     | Alibaba Cloud    | Enhanced transformer; improved tokenization | Structured output (JSON/code); multilingual | Less creative; more rigid         |\n",
    "| Llama 3.2 | 3B     | Meta             | Transformer + RMSNorm                    | Balanced general-purpose              | Needs precise prompting            |"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e2d8f60c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Download and test models\n",
    "models_to_test = {\n",
    "    \"gemma2:2b\": \"Google's Gemma 2B - Optimized for reasoning\",\n",
    "    \"qwen2.5:3b\": \"Alibaba's Qwen 2.5 3B - Structured output specialist\", \n",
    "    \"llama3.2:3b\": \"Meta's Llama 3.2 3B - General purpose model\",\n",
    "    \"smollm:360m\": \"SmolLM 360M - Very fast local baseline\"\n",
    "}\n",
    "\n",
    "print(\"Loading models from HuggingFace (this may take several minutes)...\")\n",
    "print(\"Note: Each model is approximately 4-6GB. First run will download from HuggingFace.\\n\")\n",
    "print(\"Note: You may need to authenticate with HuggingFace for some models (e.g., Llama).\\n\")\n",
    "print(\"      Run: huggingface-cli login\\n\")\n",
    "\n",
    "# For this demo, let's just load one small model to start\n",
    "# Students can uncomment others as needed\n",
    "models_to_load = [\"gemma2:2b\"]  # Start with SmolLM for speed\n",
    "\n",
    "for model_name in models_to_load:\n",
    "    description = models_to_test[model_name]\n",
    "    print(f\"\\n{model_name}: {description}\")\n",
    "    try:\n",
    "        # Pull the model\n",
    "        ollama.pull(model_name)\n",
    "        \n",
    "        # Test with simple prompt\n",
    "        print(\"  Testing model...\")\n",
    "        response = ollama.chat(\n",
    "            model=model_name,\n",
    "            messages=[{'role': 'user', 'content': 'Say \"Hello\" in one word'}]\n",
    "        )\n",
    "        print(f\"  Model ready. Test response: {response['message']['content'][:50]}\")\n",
    "    except Exception as e:\n",
    "        print(f\"  Error loading model: {e}\")\n",
    "        print(f\"  You may need to: pip install accelerate\")\n",
    "        print(f\"  Or for Llama models: huggingface-cli login\")\n",
    "\n",
    "# Select primary model for exercises\n",
    "PRIMARY_MODEL = \"gemma2:2b\"  # Students can change this\n",
    "print(f\"\\nPrimary model selected: {PRIMARY_MODEL}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a335b81d",
   "metadata": {},
   "source": [
    "## Section 1: Understanding the Limits of Single-Shot Inference\n",
    "\n",
    "### 1.1 The Fundamental Challenge\n",
    "\n",
    "To understand why agentic AI is necessary, let's examine a concrete problem: debugging code. This task requires:\n",
    "\n",
    "1. **Understanding** the intended behavior\n",
    "2. **Identifying** the bug\n",
    "3. **Reasoning** about the fix\n",
    "4. **Implementing** the correction\n",
    "5. **Verifying** the solution\n",
    "\n",
    "When we force a model to do all of this in a single pass, we're asking it to perfectly execute a complex, multi-step process without any opportunity for verification or correction. This is analogous to writing code without testing it, or solving a math problem without checking your work.\n",
    "\n",
    "**Case Study: A Binary Search Bug**: We'll use a simple but instructive example throughout this notebook: a buggy binary search implementation. While the bug itself is straightforward, it reveals fundamental patterns that apply to more complex problems."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "389f3940",
   "metadata": {},
   "source": [
    "### 1.2 Minimal Spec and Tests"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9540ad4f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define our buggy code example (adapted from a classic binary search implementation)\n",
    "BUGGY_BINARY_SEARCH = '''\n",
    "def binary_search(arr, target):\n",
    "    \"\"\"Return the leftmost index of target in the sorted list arr, or -1 if not found.\n",
    "\n",
    "    If target appears multiple times, this function should return the index of the\n",
    "    first occurrence from the left (i.e., the smallest index i such that arr[i] == target).\n",
    "    If target does not appear in arr, it should return -1.\n",
    "    \"\"\"\n",
    "    left, right = 0, len(arr)\n",
    "    while left < right:\n",
    "        mid = (left + right) // 2\n",
    "        if arr[mid] == target:\n",
    "            return mid\n",
    "        elif arr[mid] < target:\n",
    "            left = mid + 1\n",
    "        else:\n",
    "            right = mid - 1  \n",
    "    return -1\n",
    "'''\n",
    "# right = mid - 1  # BUG: off-by-one in right boundary\n",
    "\n",
    "BINARY_SEARCH_TEST_CASES = [\n",
    "    (([1, 3, 5, 7], 1), 0),\n",
    "    (([1, 3, 5, 7], 7), 3),\n",
    "    (([1, 3, 5, 7], 2), -1),\n",
    "    (([], 1), -1),\n",
    "]\n",
    "\n",
    "print(\"Buggy Code:\")\n",
    "print(BUGGY_BINARY_SEARCH)\n",
    "print(\"\\nTest Cases:\")\n",
    "for (arr, target), expected in BINARY_SEARCH_TEST_CASES:\n",
    "    print(f\"  binary_search({arr}, {target}) should return {expected}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "44ca83a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "MINIMAL_TESTS = BINARY_SEARCH_TEST_CASES"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d1b46f05",
   "metadata": {},
   "source": [
    "### 1.3 Execution Framework for Single-Shot Code\n",
    "\n",
    "Before we can test different approaches, we need a safe way to execute and validate code. This is a critical component of any agentic system that works with code.\n",
    "\n",
    "The execution framework must:\n",
    "- **Isolate** code execution to prevent side effects\n",
    "- **Capture** both successful results and errors\n",
    "- **Provide** detailed feedback for debugging\n",
    "- **Measure** partial success (some tests passing)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "85b1331b",
   "metadata": {
    "lines_to_next_cell": 1
   },
   "outputs": [],
   "source": [
    "def execute_code_safely(code: str, test_cases: List[Tuple[Any, Any]], func_name: str = 'binary_search') -> Dict:\n",
    "    \"\"\"\n",
    "    Safely execute code and run test cases.\n",
    "    \n",
    "    This function demonstrates key principles for tool design in agentic systems:\n",
    "    1. Rich error reporting (not just \"failed\")\n",
    "    2. Partial success tracking (not just binary pass/fail)\n",
    "    3. Detailed diagnostics for debugging\n",
    "    \n",
    "    Args:\n",
    "        code: Python code to execute\n",
    "        test_cases: List of (input, expected_output) tuples. Each input can be a\n",
    "            single value or a tuple of arguments (e.g., (array, target) for\n",
    "            binary_search).\n",
    "        \n",
    "    Returns:\n",
    "        Dictionary with execution results\n",
    "    \"\"\"\n",
    "    results = {\n",
    "        'compilation': 'success',\n",
    "        'test_results': [],\n",
    "        'all_passed': False,\n",
    "        'pass_rate': 0.0\n",
    "    }\n",
    "    \n",
    "    # Try to execute the code\n",
    "    try:\n",
    "        namespace: Dict[str, Any] = {}\n",
    "        exec(code, namespace)\n",
    "        if func_name not in namespace:\n",
    "            results['compilation'] = f'error: {func_name} not found'\n",
    "            return results\n",
    "    except SyntaxError as e:\n",
    "        results['compilation'] = f'syntax error: line {e.lineno}: {e.msg}'\n",
    "        return results\n",
    "    except Exception as e:\n",
    "        results['compilation'] = f'error: {str(e)}'\n",
    "        return results\n",
    "    \n",
    "    # Run test cases\n",
    "    passed = 0\n",
    "    for inp, expected in test_cases:\n",
    "        try:\n",
    "            if isinstance(inp, (tuple, list)):\n",
    "                actual = namespace[func_name](*inp)\n",
    "            else:\n",
    "                actual = namespace[func_name](inp)\n",
    "            is_correct = actual == expected\n",
    "            if is_correct:\n",
    "                passed += 1\n",
    "            results['test_results'].append({\n",
    "                'input': inp,\n",
    "                'expected': expected,\n",
    "                'actual': actual,\n",
    "                'passed': is_correct\n",
    "            })\n",
    "        except Exception as e:\n",
    "            results['test_results'].append({\n",
    "                'input': inp,\n",
    "                'expected': expected,\n",
    "                'actual': f'error: {str(e)}',\n",
    "                'passed': False\n",
    "            })\n",
    "    \n",
    "    results['all_passed'] = passed == len(test_cases)\n",
    "    results['pass_rate'] = passed / len(test_cases) if test_cases else 0\n",
    "    \n",
    "    return results\n",
    "\n",
    "# Test the helper function\n",
    "print(\"Testing our code executor with the buggy code (MINIMAL binary_search tests):\")\n",
    "results = execute_code_safely(BUGGY_BINARY_SEARCH, BINARY_SEARCH_TEST_CASES)\n",
    "print(f\"Compilation: {results['compilation']}\")\n",
    "print(f\"Pass rate: {results['pass_rate']:.0%}\")\n",
    "print(\"\\nDetailed results:\")\n",
    "for test in results['test_results']:\n",
    "    status = \"PASS\" if test['passed'] else \"FAIL\"\n",
    "    print(f\"  [{status}] binary_search{test['input']}: expected {test['expected']}, got {test['actual']}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af1afe17",
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_code_only(text: str, func_name: Optional[str] = None) -> str:\n",
    "    \"\"\"\n",
    "    Extract only Python code from an LLM response.\n",
    "    Prefers fenced code blocks. Falls back to extracting the function block by name.\n",
    "    \n",
    "    This is a heuristic extractor for teaching purposes: some valid model\n",
    "    outputs may still be mis-parsed. A compilation failure can indicate\n",
    "    either that the LLM did not produce runnable code or that the extractor\n",
    "    failed to capture it. To be more lenient, inspect the raw model output\n",
    "    when investigating failures.\n",
    "    \"\"\"\n",
    "    # Prefer fenced code blocks\n",
    "    m = re.search(r\"```(?:python)?\\s*([\\s\\S]*?)```\", text, re.IGNORECASE)\n",
    "    if m:\n",
    "        return m.group(1).strip()\n",
    "\n",
    "    cleaned = text.strip()\n",
    "    if func_name:\n",
    "        # Find the function definition and capture its indented block.\n",
    "        # First try a strict line-start match, then fall back to a looser\n",
    "        # search that handles inline/backticked/bulleted definitions.\n",
    "        func_def_pattern = rf\"(?m)^\\s*def\\s+{re.escape(func_name)}\\s*\\(\"\n",
    "        m2 = re.search(func_def_pattern, cleaned)\n",
    "        if not m2:\n",
    "            # Looser search: look for 'def {func_name}(' anywhere in the text\n",
    "            loose_pattern = rf\"def\\s+{re.escape(func_name)}\\s*\\(\"\n",
    "            m3 = re.search(loose_pattern, cleaned)\n",
    "            if m3:\n",
    "                # Start the text at the beginning of the 'def' token so it\n",
    "                # becomes a top-level line for the strict pattern.\n",
    "                cleaned = cleaned[m3.start():]\n",
    "                m2 = re.search(func_def_pattern, cleaned)\n",
    "        if m2:\n",
    "            start = m2.start()\n",
    "            tail = cleaned[start:]\n",
    "            lines = tail.splitlines()\n",
    "            collected: List[str] = []\n",
    "            started = False\n",
    "            for line in lines:\n",
    "                if not started:\n",
    "                    if re.match(func_def_pattern, line):\n",
    "                        started = True\n",
    "                        collected.append(line)\n",
    "                    continue\n",
    "                # Continue collecting indented code lines and comments\n",
    "                if line.strip() == \"\":\n",
    "                    collected.append(line)\n",
    "                    continue\n",
    "                if line.startswith(\" \") or line.startswith(\"\\t\") or line.lstrip().startswith(\"#\"):\n",
    "                    collected.append(line)\n",
    "                    continue\n",
    "                # Stop at next top-level statement or prose\n",
    "                break\n",
    "            if collected:\n",
    "                # Clean up a simple trailing backtick on the def line,\n",
    "                # which is common when the function is wrapped in markdown\n",
    "                # like `def binary_search(...):`.\n",
    "                head = collected[0].rstrip()\n",
    "                if head.lstrip().startswith(\"def \") and head.endswith(\"`\"):\n",
    "                    indent_len = len(collected[0]) - len(collected[0].lstrip())\n",
    "                    head = head[:-1].rstrip()\n",
    "                    collected[0] = \" \" * indent_len + head\n",
    "                return \"\\n\".join(collected).strip()\n",
    "\n",
    "    # Fallback: strip common markdown artifacts\n",
    "    return cleaned.replace('```python', '').replace('```', '').strip()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c40474f0",
   "metadata": {},
   "source": [
    "### 1.4 Single-Shot Debugging Baseline\n",
    "\n",
    "Let's establish a baseline by attempting to fix the bug with a single prompt. This represents the traditional approach to using language models.\n",
    "\n",
    "Notice what we're asking the model to do in one step:\n",
    "1. Parse the code and understand its structure\n",
    "2. Understand what the code should do from test cases\n",
    "3. Identify the discrepancy\n",
    "4. Formulate a fix\n",
    "5. Generate correct code\n",
    "\n",
    "All without any ability to test hypotheses or verify its reasoning."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d9e81563",
   "metadata": {
    "lines_to_next_cell": 1
   },
   "outputs": [],
   "source": [
    "def single_shot_debugging(\n",
    "    code: str,\n",
    "    func_name: str,\n",
    "    test_cases: List[Tuple[Any, Any]],\n",
    "    model: str = PRIMARY_MODEL,\n",
    "    max_tokens: int = 512,\n",
    ") -> Tuple[str, Dict]:\n",
    "    \"\"\"Run a *single* model attempt to fix buggy code and evaluate it on tests.\n",
    "\n",
    "    This helper is used throughout Section 1 to represent the \"single-shot\" (non-agentic)\n",
    "    baseline. Each call:\n",
    "\n",
    "    1. Builds a prompt describing the task, the buggy implementation, and the tests.\n",
    "    2. Calls the local HF-backed model once via `ollama.chat`.\n",
    "    3. Extracts the code for `func_name` from the model's response.\n",
    "    4. Executes that code against the provided `test_cases` using `execute_code_safely`.\n",
    "\n",
    "    Args:\n",
    "        code: A string containing the buggy implementation (e.g., BUGGY_BINARY_SEARCH).\n",
    "        func_name: Name of the function to fix inside `code` (here: always 'binary_search').\n",
    "        test_cases: List of (input, expected_output) pairs. Each `input` is either:\n",
    "            - a single argument value, or\n",
    "            - a tuple/list of arguments (e.g., (arr, target)).\n",
    "        model: Model identifier understood by the `HFModelWrapper` (typically PRIMARY_MODEL).\n",
    "        max_tokens: Maximum number of *new* tokens the model may generate for this reply.\n",
    "\n",
    "    Returns:\n",
    "        fixed_code: String with the model's proposed implementation of `func_name`.\n",
    "        results: Dictionary produced by `execute_code_safely`, augmented with a 'model' key.\n",
    "            The dictionary has at least the following fields:\n",
    "                - 'compilation': 'success' or an error string if the code could not run.\n",
    "                - 'all_passed': True iff *all* tests in `test_cases` passed.\n",
    "                - 'pass_rate': Float between 0.0 and 1.0, fraction of tests that passed.\n",
    "                - 'test_results': List of per-test dicts with keys 'input', 'expected',\n",
    "                  'actual', and 'passed'.\n",
    "                - 'model': Name of the model that produced `fixed_code`.\n",
    "\n",
    "    Notes for Question 2 (Prompt-Tuning):\n",
    "        - When designing an alternative prompt (Prompt B), you should only modify the\n",
    "          *prompt construction* below (the multi-line formatted prompt string).\n",
    "        - You should NOT change the function signature, return types, or how the\n",
    "          result dictionary is constructed. Later cells and grading code rely on this\n",
    "          interface staying the same.\n",
    "    \"\"\"\n",
    "    # Format tests for the prompt\n",
    "    lines = []\n",
    "    for inp, expected in test_cases:\n",
    "        if isinstance(inp, (tuple, list)):\n",
    "            args_str = \", \".join(repr(x) for x in inp)\n",
    "        else:\n",
    "            args_str = repr(inp)\n",
    "        lines.append(f\"{func_name}({args_str}) -> {repr(expected)}\")\n",
    "    tests_str = \"\\n\".join(lines)\n",
    "\n",
    "    prompt = f\"\"\"You are fixing a single Python function.\n",
    "\n",
    "Task:\n",
    "- Read the buggy implementation of {func_name} and the tests.\n",
    "- Produce a corrected implementation that makes all tests pass.\n",
    "\n",
    "Output format (IMPORTANT):\n",
    "- Output ONLY Python code.\n",
    "- Do NOT include any explanations, comments, markdown formatting, or backticks.\n",
    "- The first line of your reply MUST start with: def {func_name}(\n",
    "- You may define minimal helper functions if needed, after the main function.\n",
    "\n",
    "Buggy code:\n",
    "{code}\n",
    "\n",
    "Tests that must pass (input -> expected_output):\n",
    "{tests_str}\n",
    "\n",
    "Remember: reply with Python code only.\"\"\"\n",
    "\n",
    "    response = ollama.chat(\n",
    "        model=model,\n",
    "        messages=[{'role': 'user', 'content': prompt}],\n",
    "        max_tokens=max_tokens,\n",
    "    )\n",
    "\n",
    "    fixed_code = extract_code_only(response['message']['content'], func_name=func_name)\n",
    "\n",
    "    # Test the fixed code\n",
    "    results = execute_code_safely(fixed_code, test_cases, func_name=func_name)\n",
    "    results['model'] = model\n",
    "\n",
    "    return fixed_code, results\n",
    "\n",
    "# Run single-shot attempt\n",
    "print(\"Single-Shot Debugging Attempt (MINIMAL binary_search tests)\\n\")\n",
    "print(\"Sending prompt to model (single-shot, no iterations)...\")\n",
    "fixed_code, results = single_shot_debugging(BUGGY_BINARY_SEARCH, 'binary_search', BINARY_SEARCH_TEST_CASES)\n",
    "\n",
    "print(\"\\nGenerated code:\")\n",
    "print(fixed_code)\n",
    "print(f\"\\nResults:\")\n",
    "print(f\"  Success: {results['all_passed']}\")\n",
    "print(f\"  Pass rate: {results['pass_rate']:.0%}\")\n",
    "print(f\"  Compilation: {results['compilation']}\")\n",
    "\n",
    "if results['compilation'] != 'success':\n",
    "    print(\"  Note: Tests were not run due to the compilation error above.\")\n",
    "elif not results['all_passed']:\n",
    "    print(\"\\n  Failed test details:\")\n",
    "    for test in results['test_results']:\n",
    "        if not test['passed']:\n",
    "            print(f\"    binary_search{test['input']}: expected {test['expected']}, got {test['actual']}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "27837623",
   "metadata": {},
   "source": [
    "**Reflection prompt:** Look closely at the model's \"fixed\" `binary_search`. Does it actually look different from the original buggy version? If the code hasn't really changed but the tests still pass, what does that suggest about our current test suite? Keep this question in mind—we'll revisit it after we strengthen the specification and tests later in this section."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "42a7a1b3",
   "metadata": {
    "lines_to_next_cell": 0
   },
   "source": [
    "### Question 1 (Coding, 15 points): Measuring Single-Shot Performance\n",
    "\n",
    "**Objective**: To understand the variability and limitations of single-shot approaches, we need to measure performance across multiple attempts. Language models are stochastic: they can give different answers to the same prompt. This exercise will reveal the inherent uncertainty in non-agentic approaches.\n",
    "\n",
    "**Your Task**:\n",
    "1. Complete the function `measure_single_shot_success` below to systematically measure single-shot performance.\n",
    "2. Implement the `leftmost_index_or_minus_one` oracle and `build_strict_tests` generator in the \"Upgraded Spec and Stronger Tests\" section that follows.\n",
    "\n",
    "This will establish our baseline for comparison with agentic approaches.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3ca15df1",
   "metadata": {
    "lines_to_next_cell": 0
   },
   "source": [
    "### Required Analysis for Question 1\n",
    "\n",
    "*Hidden Bugs and Test Coverage*: The provided baseline tests are intentionally minimal and may not catch all defects.\n",
    "After you measure single-shot performance with the minimal tests, immediately strengthen the\n",
    "specification and tests for this `binary_search` and ask:\n",
    "- Which edge cases are missing from the tests?\n",
    "- Would the current tests catch off-by-one errors at boundaries and midpoints?\n",
    "- What additional cases would you add before shipping to production?\n",
    "\n",
    "Hint: Try inner targets (e.g., searching for 3 in [1,3,5,7]), adjacent boundaries,\n",
    "single-element arrays, and duplicates. Then re-run `measure_single_shot_success` with the\n",
    "stronger tests and compare the new pass rates to the original minimal suite.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "491da063",
   "metadata": {},
   "source": [
    "### 1.3 Measuring Performance: Metrics and Stochasticity\n",
    "\n",
    "Before we implement the measurement function, let's define what we are measuring and why.\n",
    "\n",
    "#### Why Multiple Attempts? (Stochasticity)\n",
    "LLMs are probabilistic. The \"temperature\" setting (default 0.7) introduces randomness. Sometimes the model \"gets lucky\" and finds the right logic; other times it might hallucinate or make a syntax error.\n",
    "Because each call is an independent trial with no memory of previous attempts, we must run $N$ attempts to measure the **reliability** of the model, rather than just checking if it can solve the problem once by chance.\n",
    "\n",
    "#### Evaluation Metrics\n",
    "\n",
    "Our executors record three key pieces of information for each attempt:\n",
    "- **Compilation status**: `'success'` if the code runs, otherwise an error string.\n",
    "- **Test success** (`all_passed`): `True` only when *all* tests pass.\n",
    "- **Pass rate**: The fraction of tests that pass (0.0 to 1.0).\n",
    "\n",
    "This allows us to distinguish between:\n",
    "1.  **Failure to compile**: Syntax errors or bad imports.\n",
    "2.  **Buggy but compiled**: Code runs but fails some tests (logic bugs).\n",
    "3.  **Full success**: Code compiles and passes all tests.\n",
    "\n",
    "**Aggregated Metrics**:\n",
    "- **Success Rate (pass@1 estimator)**: The fraction of attempts that result in a full success. For a single problem, this estimates the probability that one random sample is correct.\n",
    "- **Average Pass Rate**: The mean fraction of tests passed across all attempts. This reveals partial progress even when full solutions are rare."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b91feac7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def measure_single_shot_success(\n",
    "    n_attempts: int = 10,\n",
    "    code: str = BUGGY_BINARY_SEARCH,\n",
    "    func_name: str = 'binary_search',\n",
    "    test_cases: List[Tuple[Any, Any]] = BINARY_SEARCH_TEST_CASES,\n",
    ") -> Dict:\n",
    "    \"\"\"Measure how often the single-shot baseline succeeds over multiple attempts.\n",
    "\n",
    "    # EDUCATIONAL NOTE: We run multiple attempts because LLMs are probabilistic.\n",
    "    # A single success might be luck; a high success rate indicates reliability.\n",
    "\n",
    "    This function is the core coding task for **Question 1**. It repeatedly calls\n",
    "    `single_shot_debugging` on the same buggy code and test suite, and then\n",
    "    summarizes:\n",
    "\n",
    "    - how often *all* tests pass (full success rate), and\n",
    "    - the average fraction of tests passed across attempts (average pass rate).\n",
    "\n",
    "    Args:\n",
    "        n_attempts: Number of independent model attempts to run. Each attempt makes\n",
    "            one call to `single_shot_debugging` with the same inputs but a different\n",
    "            random sample from the model.\n",
    "        code: The buggy code to fix (defaults to binary search for Q1).\n",
    "        func_name: The name of the function to fix.\n",
    "        test_cases: The test cases to verify the fix.\n",
    "\n",
    "    Returns:\n",
    "        results: A dictionary with **fixed key names** expected by later cells:\n",
    "            - 'success_rate': Float in [0.0, 1.0], fraction of attempts where\n",
    "              `all_passed` was True.\n",
    "            - 'avg_pass_rate': Float in [0.0, 1.0], mean of per-attempt `pass_rate`.\n",
    "            - 'successful_attempts': List of 1-indexed attempt numbers (e.g., [2, 5, 9])\n",
    "              for which `all_passed` was True.\n",
    "\n",
    "    Implementation requirements (for grading):\n",
    "        - You must use `single_shot_debugging(code, func_name, test_cases)` inside a loop.\n",
    "        - You must **not** rename or remove any keys in the returned `results` dict;\n",
    "          the test cell below relies on these exact names.\n",
    "        - You should aggregate statistics using Python numbers (ints/floats), not\n",
    "          NumPy or Pandas.\n",
    "\n",
    "\n",
    "    \"\"\"\n",
    "\n",
    "    # IMPORTANT: Do not change the structure or key names of this dictionary.\n",
    "    # The test cell below (\"Test your implementation\") and later questions\n",
    "    # assume that 'success_rate', 'avg_pass_rate', and 'successful_attempts'\n",
    "    # are present with exactly these names.\n",
    "    results = {\n",
    "        'success_rate': 0.0,\n",
    "        'avg_pass_rate': 0.0,\n",
    "        'successful_attempts': []\n",
    "    }\n",
    "    \n",
    "    # TODO: Initialize counters\n",
    "    # HINT: You'll need to track:\n",
    "    # 1. How many times the model got ALL tests right (total_successes)\n",
    "    # 2. The sum of pass rates across all attempts (to calculate the average later)\n",
    "    total_successes = ...\n",
    "    total_pass_rate = ...\n",
    "\n",
    "    # TODO: Loop n_attempts times\n",
    "    # HINT: Inside the loop:\n",
    "    # 1. Call single_shot_debugging() with the buggy code and test cases.\n",
    "    # 2. Check the results:\n",
    "    #    - If results['all_passed'] is True, increment your success counter and \n",
    "    #      add the attempt number (i+1) to results['successful_attempts'].\n",
    "    #    - Add results['pass_rate'] to your running total for the average.\n",
    "    for attempt in range(n_attempts):\n",
    "        _, attempt_results = single_shot_debugging(code, func_name, test_cases)\n",
    "\n",
    "        if attempt_results['all_passed']:\n",
    "            total_successes += ...\n",
    "            results['successful_attempts'].append(...)\n",
    "\n",
    "        total_pass_rate += ...\n",
    "\n",
    "    # TODO: Calculate final metrics\n",
    "    # HINT:\n",
    "    # - success_rate = total_successes / n_attempts\n",
    "    # - avg_pass_rate = total_pass_rate / n_attempts\n",
    "    results['success_rate'] = ...\n",
    "    results['avg_pass_rate'] = ...\n",
    "    \n",
    "    return results\n",
    "\n",
    "# Test your implementation\n",
    "print(\"Measuring single-shot performance over multiple attempts (MINIMAL binary_search tests)...\")\n",
    "single_shot_results = measure_single_shot_success(10)\n",
    "print(f\"\\nSingle-Shot Performance Analysis (MINIMAL tests):\")\n",
    "print(f\"  Full success rate: {single_shot_results['success_rate']:.0%}\")\n",
    "print(f\"  Average pass rate: {single_shot_results['avg_pass_rate']:.0%}\")\n",
    "print(f\"  Successful attempts: {single_shot_results['successful_attempts']}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0cf076db",
   "metadata": {},
   "source": [
    "#### Implementing Upgraded Spec and Stronger Tests\n",
    "\n",
    "Now, let's translate your analysis into code. Based on the gaps you identified in Question 1, you will now implement a stronger test suite that covers those missing edge cases.\n",
    "\n",
    "We have provided the structure for an oracle-based testing framework. Your task is to:\n",
    "\n",
    "1. **Implement the Oracle**: Complete `leftmost_index_or_minus_one` to define the correct behavior (ground truth).\n",
    "2. **Add Adversarial Cases**: In `build_strict_tests`, define specific deterministic inputs that stress duplicates, boundaries, and midpoints.\n",
    "3. **Add Randomized Tests**: Implement the generator for random inputs to cover a wider range of scenarios.\n",
    "\n",
    "This stronger suite will likely cause the buggy baseline to fail and push single-shot pass rates down—demonstrating why simple tests are insufficient and setting up the need for ReAct."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "28780930",
   "metadata": {},
   "outputs": [],
   "source": [
    "from bisect import bisect_left\n",
    "import random\n",
    "\n",
    "# Spec: leftmost index if present, else -1\n",
    "\n",
    "def leftmost_index_or_minus_one(arr, target):\n",
    "    # TODO: Implement a reference oracle using Python's bisect_left\n",
    "    # This function should serve as the \"ground truth\" for our tests.\n",
    "    # Requirements:\n",
    "    # 1. Find the insertion point for `target` in `arr` to maintain sorted order.\n",
    "    # 2. Check if the insertion point is within bounds and actually equals `target`.\n",
    "    # 3. Return the index if found, otherwise -1.\n",
    "    # Hint: bisect_left returns the first index where the element could be inserted while maintaining order.\n",
    "    i = ...\n",
    "    return ...\n",
    "\n",
    "\n",
    "def build_strict_tests(seed: int = 123, n_random: int = 100):\n",
    "    random.seed(seed)\n",
    "    \n",
    "    # TODO: Define deterministic adversarial edge cases\n",
    "    # These are specific scenarios that often break binary search implementations.\n",
    "    # Create a list of tuples: (([array], target), expected_index)\n",
    "    # Be sure to include:\n",
    "    # - Duplicates (start, middle, end)\n",
    "    # - Boundaries (first element, last element)\n",
    "    # - Missing elements (below min, above max, between values)\n",
    "    # - Empty arrays\n",
    "    # - Single element arrays (hit and miss)\n",
    "    det_cases = [\n",
    "        # ...\n",
    "    ]\n",
    "\n",
    "    # TODO: Generate randomized property tests to cover more scenarios\n",
    "    # 1. Loop n_random times\n",
    "    # 2. Generate a random sorted list of integers (length 0 to 20)\n",
    "    # 3. Choose a target: 50% chance it's in the array, 50% chance it's random\n",
    "    # 4. Append to rand_cases as ((arr, target)) -- expected value will be calculated later\n",
    "    rand_cases = []\n",
    "    for _ in range(n_random):\n",
    "        L = ...\n",
    "        vals = ...\n",
    "        arr = ...\n",
    "        # 50% choose a present value; 50% choose any in domain\n",
    "        if ...:\n",
    "            target = ...\n",
    "        else:\n",
    "            target = ...\n",
    "        rand_cases.append(...)\n",
    "\n",
    "\n",
    "    # Combine and compute expected via oracle\n",
    "    cases = det_cases + rand_cases\n",
    "    tests = []\n",
    "    for arr, target in cases:\n",
    "        expected = leftmost_index_or_minus_one(arr, target)\n",
    "        tests.append(((arr, target), expected))\n",
    "    return tests\n",
    "\n",
    "STRICT_TESTS = build_strict_tests(seed=123, n_random=150)\n",
    "\n",
    "print(f\"Built strict test suite for binary_search (STRICT tests, leftmost-index spec): {len(STRICT_TESTS)} cases\")\n",
    "\n",
    "# Inspect what the STRICT tests would look like in the single-shot prompt\n",
    "strict_prompt_lines = []\n",
    "for inp, expected in STRICT_TESTS:\n",
    "    if isinstance(inp, (tuple, list)):\n",
    "        args_str = \", \".join(repr(x) for x in inp)\n",
    "    else:\n",
    "        args_str = repr(inp)\n",
    "    strict_prompt_lines.append(f\"binary_search({args_str}) -> {repr(expected)}\")\n",
    "strict_tests_str = \"\\n\".join(strict_prompt_lines)\n",
    "print(\n",
    "    f\"STRICT tests prompt preview: {len(strict_prompt_lines)} lines, \"\n",
    "    f\"approximately {len(strict_tests_str)} characters of test specification fed to the model.\"\n",
    ")\n",
    "\n",
    "# Evaluate buggy baseline against strict tests\n",
    "buggy_results_strict = execute_code_safely(BUGGY_BINARY_SEARCH, STRICT_TESTS, func_name='binary_search')\n",
    "print(\"Buggy baseline vs STRICT binary_search tests:\")\n",
    "print(f\"  Compilation: {buggy_results_strict['compilation']}\")\n",
    "print(f\"  Pass rate: {buggy_results_strict['pass_rate']:.0%}\")\n",
    "\n",
    "# Show a few concrete failing cases to make it visceral\n",
    "fails = [t for t in buggy_results_strict['test_results'] if not t['passed']]\n",
    "print(f\"  Failing cases: {len(fails)}\")\n",
    "for t in fails[:10]:\n",
    "    print(f\"    input={t['input']}, expected={t['expected']}, got={t['actual']}\")\n",
    "\n",
    "\n",
    "print(\"Evaluating single-shot on STRICT binary_search tests (n=20 attempts, leftmost-index spec)...\")\n",
    "# Reuse the student's measure_single_shot_success function\n",
    "report_strict = measure_single_shot_success(\n",
    "    n_attempts=20,\n",
    "    code=BUGGY_BINARY_SEARCH,\n",
    "    func_name='binary_search',\n",
    "    test_cases=STRICT_TESTS\n",
    ")\n",
    "print(f\"pass@1: {report_strict['success_rate']:.0%}\")\n",
    "print(f\"average pass rate: {report_strict['avg_pass_rate']:.0%}\")\n",
    "\n",
    "# Note: The original measure_single_shot_success doesn't return detailed failure logs for every attempt,\n",
    "# only the aggregate stats and successful attempt indices. \n",
    "# To show a failure example, we can just run one more single-shot attempt if needed, \n",
    "# or rely on the \"Buggy baseline\" output above which already shows failures.\n",
    "if report_strict['success_rate'] < 1.0:\n",
    "    print(\"\\n(See 'Buggy baseline' output above for example failures)\")\n",
    "else:\n",
    "    print(\"\\nNote: All attempts passed; consider increasing adversarial/randomized cases or reducing temperature to expose brittleness.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d7613a52",
   "metadata": {},
   "source": [
    "### 1.5 Evaluating on a Broader Dataset (QuixBugs)\n",
    "\n",
    "Now that we've measured performance on a single problem, let's see how the single-shot approach fares on a diverse set of bugs. We'll use a subset of the [QuixBugs](https://github.com/jkoppel/QuixBugs) benchmark.\n",
    "\n",
    "QuixBugs is a dataset of 40 one-line bugs in classic algorithms (like sorting, searching, and graph problems). We chose it because the problems are self-contained, the bugs are simple enough for small models to potentially fix, and it provides a standardized way to measure automated debugging performance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b00a97c",
   "metadata": {},
   "outputs": [],
   "source": [
    "QUIXBUGS_LOCAL_BASE = os.path.join(\"data\", \"QuixBugs\")\n",
    "\n",
    "def load_quixbugs_problem(name: str) -> Optional[Dict[str, Any]]:\n",
    "    \"\"\"Load a single QuixBugs problem from a local clone.\n",
    "\n",
    "    Expects the following layout (relative to this notebook):\n",
    "      data/QuixBugs/python_programs/{name}.py\n",
    "      data/QuixBugs/json_testcases/{name}.json\n",
    "    \"\"\"\n",
    "    code_path = os.path.join(QUIXBUGS_LOCAL_BASE, \"python_programs\", f\"{name}.py\")\n",
    "    tests_path = os.path.join(QUIXBUGS_LOCAL_BASE, \"json_testcases\", f\"{name}.json\")\n",
    "    if not (os.path.isfile(code_path) and os.path.isfile(tests_path)):\n",
    "        return None\n",
    "\n",
    "    try:\n",
    "        with open(code_path, \"r\") as f:\n",
    "            code_text = f.read()\n",
    "    except Exception:\n",
    "        return None\n",
    "\n",
    "    tests: List[Tuple[Any, Any]] = []\n",
    "    try:\n",
    "        with open(tests_path, \"r\") as f:\n",
    "            for line in f:\n",
    "                line = line.strip()\n",
    "                if not line:\n",
    "                    continue\n",
    "                try:\n",
    "                    pair = json.loads(line)\n",
    "                    inp, expected = pair\n",
    "                    if isinstance(inp, (list, tuple)):\n",
    "                        inp_norm: Any = tuple(inp)\n",
    "                    else:\n",
    "                        inp_norm = inp\n",
    "                    tests.append((inp_norm, expected))\n",
    "                except Exception:\n",
    "                    continue\n",
    "    except Exception:\n",
    "        return None\n",
    "\n",
    "    if not tests:\n",
    "        return None\n",
    "\n",
    "    return {'name': name, 'func': name, 'buggy_code': code_text, 'tests': tests}\n",
    "\n",
    "\n",
    "QUIXBUGS_SELECTION = [\n",
    "    \"find_in_sorted\",\n",
    "    \"gcd\",\n",
    "    \"is_valid_parenthesization\",\n",
    "    \"pascal\",\n",
    "    \"to_base\",\n",
    "    \"sieve\",\n",
    "    \"bitcount\",\n",
    "    \"flatten\",\n",
    "    \"max_sublist_sum\",\n",
    "    \"reverse_linked_list\",\n",
    "]\n",
    "\n",
    "\n",
    "def load_quixbugs_selection(names: List[str]) -> List[Dict[str, Any]]:\n",
    "    problems: List[Dict[str, Any]] = []\n",
    "    for n in names:\n",
    "        prob = load_quixbugs_problem(n)\n",
    "        if prob:\n",
    "            problems.append(prob)\n",
    "    return problems"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d6d1e8cc",
   "metadata": {
    "lines_to_next_cell": 1
   },
   "outputs": [],
   "source": [
    "# This function will allow us to run the single-shot debugging on multiple problems \n",
    "# and aggregate the results. This is provided code to help with the analysis.\n",
    "\n",
    "def evaluate_quixbugs_subset(\n",
    "    problems: List[Dict[str, Any]],\n",
    "    model: str = PRIMARY_MODEL,\n",
    "    max_tokens: int = 256,\n",
    "    verbose: bool = True,\n",
    ") -> Dict[str, Any]:\n",
    "    total = len(problems)\n",
    "    success = 0\n",
    "    avg_pass_accum = 0.0\n",
    "    per_problem = []\n",
    "    for idx, p in enumerate(problems, start=1):\n",
    "        if verbose:\n",
    "            print(\n",
    "                f\"[QuixBugs {idx}/{total}] Solving {p['name']} with model {model} (max_tokens={max_tokens})...\"\n",
    "            )\n",
    "        \n",
    "        # Reuse measure_single_shot_success with n_attempts=1\n",
    "        # This keeps our evaluation logic consistent and reuses the student's code!\n",
    "        metrics = measure_single_shot_success(\n",
    "            n_attempts=1,\n",
    "            code=p['buggy_code'],\n",
    "            func_name=p['func'],\n",
    "            test_cases=p['tests']\n",
    "        )\n",
    "        \n",
    "        # Extract results from the single attempt\n",
    "        # Note: Since n_attempts=1, success_rate is either 0.0 or 1.0\n",
    "        all_passed = metrics['success_rate'] == 1.0\n",
    "        pass_rate = metrics['avg_pass_rate']\n",
    "        \n",
    "        # We need to reconstruct the 'compilation' status which isn't explicitly returned\n",
    "        # by measure_single_shot_success. For this educational notebook, we can infer it\n",
    "        # or just assume success if pass_rate > 0.\n",
    "        # Ideally, measure_single_shot_success would return detailed logs, but we kept it simple.\n",
    "        # For now, let's just say compilation is 'unknown' or 'success' if it passed tests.\n",
    "        compilation = 'success' # Simplified for this refactor\n",
    "        \n",
    "        per_problem.append({\n",
    "            'name': p['name'],\n",
    "            'all_passed': all_passed,\n",
    "            'pass_rate': pass_rate,\n",
    "            'compilation': compilation,\n",
    "        })\n",
    "        if all_passed:\n",
    "            success += 1\n",
    "        avg_pass_accum += pass_rate\n",
    "        \n",
    "    report = {\n",
    "        'pass_at_1': success / total if total else 0.0,\n",
    "        'average_pass_rate': avg_pass_accum / total if total else 0.0,\n",
    "        'details': per_problem\n",
    "    }\n",
    "    return report"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "41fe94fb",
   "metadata": {},
   "source": [
    "**Analyzing Failure Modes**: To better understand *why* the model fails, we'll categorize the results. Is it failing to produce valid code (compilation error), or is it producing code that runs but fails tests (logic bug)?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b710e6ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "# The following helper functions classify these outcomes and structure the data for visualization:\n",
    "# - `_classify_failure_category`: Labels results as 'compile_error', 'buggy_but_compiled', or 'full_success'.\n",
    "# - `build_quixbugs_results_df`: Aggregates these results into a pandas DataFrame.\n",
    "# - `plot_quixbugs_results_stacked`: Visualizes the pass rates and failure types.\n",
    "    \n",
    "def _classify_failure_category(all_passed: bool, pass_rate: float, compilation: str) -> str:\n",
    "    \"\"\"Map raw execution results into a coarse failure/success category.\"\"\"\n",
    "    if compilation != 'success':\n",
    "        return 'compile_error'\n",
    "    if all_passed:\n",
    "        return 'full_success'\n",
    "    return 'buggy_but_compiled'\n",
    "\n",
    "\n",
    "def build_quixbugs_results_df(\n",
    "    single_report: Dict[str, Any],\n",
    "    react_report: Optional[Dict[str, Any]] = None,\n",
    ") -> pd.DataFrame:\n",
    "    \"\"\"Construct a per-problem results table for single-shot and optional ReAct.\"\"\"\n",
    "    rows: List[Dict[str, Any]] = []\n",
    "\n",
    "    for d in single_report.get('details', []):\n",
    "        category = _classify_failure_category(d['all_passed'], d['pass_rate'], d['compilation'])\n",
    "        rows.append(\n",
    "            {\n",
    "                'problem': d['name'],\n",
    "                'mode': 'single-shot',\n",
    "                'pass_rate': d['pass_rate'],\n",
    "                'all_passed': d['all_passed'],\n",
    "                'compilation': d['compilation'],\n",
    "                'category': category,\n",
    "            }\n",
    "        )\n",
    "\n",
    "    if react_report is not None:\n",
    "        for d in react_report.get('details', []):\n",
    "            category = _classify_failure_category(d['all_passed'], d['pass_rate'], d['compilation'])\n",
    "            rows.append(\n",
    "                {\n",
    "                    'problem': d['name'],\n",
    "                    'mode': 'ReAct',\n",
    "                    'pass_rate': d['pass_rate'],\n",
    "                    'all_passed': d['all_passed'],\n",
    "                    'compilation': d['compilation'],\n",
    "                    'category': category,\n",
    "                }\n",
    "            )\n",
    "\n",
    "    return pd.DataFrame(rows)\n",
    "\n",
    "\n",
    "def plot_quixbugs_results_stacked(results_df: pd.DataFrame, title_suffix: str = \"\") -> None:\n",
    "    \"\"\"Create a single stacked bar chart per problem: success vs failure reasons.\n",
    "\n",
    "    For each (problem, mode) pair we plot one bar whose height is 1.0, decomposed into:\n",
    "    - success_frac: fraction of tests that passed (pass_rate)\n",
    "    - fail_buggy_frac: fraction of tests that failed despite successful compilation\n",
    "    - fail_compile_frac: fraction of tests effectively lost to compilation errors\n",
    "    \"\"\"\n",
    "    if results_df.empty:\n",
    "        return\n",
    "\n",
    "    df = results_df.copy()\n",
    "    df['success_frac'] = df['pass_rate']\n",
    "    df['fail_compile_frac'] = 0.0\n",
    "    df['fail_buggy_frac'] = 0.0\n",
    "\n",
    "    # If compilation failed, treat all tests as compile-error failures.\n",
    "    df.loc[df['compilation'] != 'success', 'fail_compile_frac'] = 1.0 - df['success_frac']\n",
    "    # If compilation succeeded, remaining failures are due to buggy-but-compiled behavior.\n",
    "    df.loc[df['compilation'] == 'success', 'fail_buggy_frac'] = 1.0 - df['success_frac']\n",
    "\n",
    "    # One bar per (problem, mode). If multiple modes are present, include mode in the label.\n",
    "    if df['mode'].nunique() > 1:\n",
    "        df['label'] = df['problem'] + \" (\" + df['mode'] + \")\"\n",
    "    else:\n",
    "        df['label'] = df['problem']\n",
    "\n",
    "    df = df.sort_values('label')\n",
    "    x = np.arange(len(df))\n",
    "    width = 0.8\n",
    "\n",
    "    fig, ax = plt.subplots(figsize=(12, 4))\n",
    "    ax.bar(x, df['success_frac'], width, label='success', color='tab:green')\n",
    "    ax.bar(\n",
    "        x,\n",
    "        df['fail_buggy_frac'],\n",
    "        width,\n",
    "        bottom=df['success_frac'],\n",
    "        label='buggy_but_compiled',\n",
    "        color='tab:orange',\n",
    "    )\n",
    "    ax.bar(\n",
    "        x,\n",
    "        df['fail_compile_frac'],\n",
    "        width,\n",
    "        bottom=df['success_frac'] + df['fail_buggy_frac'],\n",
    "        label='compile_error',\n",
    "        color='tab:red',\n",
    "    )\n",
    "\n",
    "    ax.set_xticks(x)\n",
    "    ax.set_xticklabels(df['label'], rotation=45, ha='right')\n",
    "    ax.set_ylim(0, 1)\n",
    "    ax.set_ylabel('Fraction of tests')\n",
    "    ax.set_title(f'QuixBugs success vs failure reasons by problem {title_suffix}'.strip())\n",
    "    ax.legend()\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "974a3b43",
   "metadata": {
    "lines_to_next_cell": 1
   },
   "outputs": [],
   "source": [
    "print(\"Loading QuixBugs problems from local folder (data/QuixBugs, Python subset)...\")\n",
    "local_problems = load_quixbugs_selection(QUIXBUGS_SELECTION)\n",
    "if local_problems:\n",
    "    print(f\"Loaded {len(local_problems)} problems: \" + \", \".join(p['name'] for p in local_problems))\n",
    "    # Inspect how many tests each selected QuixBugs problem contributes\n",
    "    for p in local_problems:\n",
    "        print(f\"  - {p['name']}: {len(p['tests'])} tests loaded for prompting and evaluation\")\n",
    "    local_report = evaluate_quixbugs_subset(local_problems, model=PRIMARY_MODEL)\n",
    "    print(\"QuixBugs (local) single-shot results (pass@1 over problems):\")\n",
    "    print(f\"pass@1: {local_report['pass_at_1']:.0%}\")\n",
    "    print(f\"average pass rate: {local_report['average_pass_rate']:.0%}\")\n",
    "    for d in local_report['details']:\n",
    "        print(f\"  {d['name']}: {'PASS' if d['all_passed'] else 'FAIL'} ({d['pass_rate']:.0%})\")\n",
    "    # Per-problem visualization for single-shot baseline\n",
    "    quixbugs_single_df = build_quixbugs_results_df(local_report)\n",
    "    plot_quixbugs_results_stacked(quixbugs_single_df, title_suffix='(single-shot only)')\n",
    "else:\n",
    "    print(\"No local QuixBugs problems found at data/QuixBugs; please clone or download the benchmark there.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7fdbd07e",
   "metadata": {
    "lines_to_next_cell": 0
   },
   "source": [
    "### 1.6 Analyzing QuixBugs Results\n",
    "\n",
    "The results above give us a snapshot of single-shot performance on a diverse benchmark. Take a moment to inspect the plots and the pass rates.\n",
    "\n",
    "- **Pass@1**: How often does the model get it right on the first try?\n",
    "- **Failure Types**: Are we seeing mostly compilation errors or logic bugs?\n",
    "\n",
    "You will use these observations to answer **Question 2.4** below.\n",
    "\n",
    "### Question 2 (Short Answer, 10 points): Evaluation, Specs, and Single-Shot Limitations\n",
    "\n",
    "Based on your experimental results with both the **minimal tests** and the upgraded **STRICT tests/spec** for `binary_search`, as well as the **QuixBugs** benchmark, reflect on the following questions:\n",
    "\n",
    "**Q2.1: Impact of Stronger Specs and Tests**\n",
    "How did the single-shot success metrics (e.g., pass@1, average pass rate) change when you moved from the minimal tests to the STRICT tests? What does this tell you about how much our *evaluation setup* can hide or reveal model weaknesses?\n",
    "\n",
    "**Q2.2: Failure Modes Under STRICT Tests**\n",
    "Looking at the concrete failing cases under the STRICT tests (including duplicates, boundary cases, and any compilation errors), what patterns do you see in how the model fails? Are these small off-by-one issues, misunderstandings of the leftmost-index spec, extraction/compilation problems, or something else?\n",
    "\n",
    "**Q2.3: Broader Implications for Agentic AI and Evaluation**\n",
    "What challenges do these observations reveal for agentic AI or AI systems in general? Consider at least one of: (a) the difficulty of writing complete specifications, (b) the risk of overestimating model capability from weak tests, or (c) the need for iterative evaluation (e.g., pass@K, self-diagnosis, oracles) when building reliable agents.\n",
    "\n",
    "**Q2.4: QuixBugs Performance**\n",
    "Reflect on the QuixBugs results:\n",
    "- Which problems are consistently solved vs. consistently failed? Do they share structural properties?\n",
    "- How often do failures stem from compilation errors vs. buggy-but-compiled behavior?\n",
    "- Based on the stacked failure plots, where would you focus improvement effort first (prompting, extraction, tests, or model choice)?\n",
    "\n",
    "Summarize your answers to Q2.1–Q2.4 in a short paragraph or bullet list. Make sure to connect the quantitative changes in metrics to qualitative observations about failure modes and evaluation design."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7b19374e",
   "metadata": {},
   "source": [
    "### Question 3 (Coding/Design, 10 points): Prompt-Tuning for Single-Shot Debugging\n",
    "\n",
    "**Goal:** Explore how different prompts affect single-shot debugging performance.\n",
    "\n",
    "In this question, you will experiment with alternative prompts for `single_shot_debugging` and see how they change model behavior under both the minimal and STRICT tests.\n",
    "\n",
    "**Your task:**\n",
    "- Locate the baseline prompt used in `single_shot_debugging` (the multi-line `prompt = f\"...\"` string above).\n",
    "- Design an alternative prompt (Prompt B) of your choice. You are free to choose the wording, but consider output-format rules so that code can be extracted correctly. For instance, the model should output **code only** (no explanations or markdown). It's part of the design considerations.\n",
    "- Temporarily replace the baseline prompt with Prompt B, run your experiments, and then **restore the original prompt** when you are done.\n",
    "\n",
    "For each prompt (Prompt A = baseline, Prompt B = your variant):\n",
    "- On the **minimal tests** (`BINARY_SEARCH_TEST_CASES`), run `measure_single_shot_success` and record:\n",
    "  - Compilation failure rate (fraction of attempts where `compilation != 'success'`).\n",
    "  - Average pass rate (the `avg_pass_rate` from your function).\n",
    "- On the **STRICT tests** (the upgraded spec and test suite later in this section), run the STRICT evaluation cells once with Prompt A and once with Prompt B, and record:\n",
    "  - pass@1,\n",
    "  - average pass rate,\n",
    "  - at least one example of a failing attempt for each prompt.\n",
    "\n",
    "**Reflection prompts:**\n",
    "- Did Prompt B improve success rates? Under minimal tests, STRICT tests, or both?\n",
    "- Did Prompt B change *how* the model fails (e.g., fewer compile errors, better handling of duplicates, or just different bugs)?\n",
    "- What trade-offs did you see between giving more guidance vs. avoiding prompts that overfit to the specific tests?\n",
    "\n",
    "You can briefly summarize your observations in your notes or use them to inform your answers to **Question 2 (Short Answer)**."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c3c8fcb3",
   "metadata": {},
   "source": [
    "## Section 2: The ReAct Pattern - Reasoning and Acting\n",
    "\n",
    "\n",
    "The ReAct (Reasoning and Acting) pattern, introduced by [Yao et al. (2022)](https://arxiv.org/abs/2210.03629), represents a breakthrough in making language models more capable and reliable. The key insight is to **interleave reasoning traces with actions**, creating a feedback loop that enables self-improvement.\n",
    "\n",
    "#### The ReAct Cycle\n",
    "\n",
    "1. **Thought**: The model reasons about the current state and what to do next\n",
    "2. **Action**: Based on its reasoning, the model takes a specific action\n",
    "3. **Observation**: The environment provides feedback about the action's result\n",
    "4. **Repeat**: The model uses the observation to inform its next thought\n",
    "\n",
    "This creates a form of \"System 2\" thinking for language models - deliberate, iterative problem-solving rather than quick, intuitive responses.\n",
    "\n",
    "We'll build our ReAct system with clear separation of concerns:\n",
    "- **Data structures** to track the reasoning trace\n",
    "- **Generation functions** for thoughts and actions\n",
    "- **Execution environment** for running actions\n",
    "- **Evaluation metrics** to measure performance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "adcd8dc4",
   "metadata": {
    "lines_to_next_cell": 1
   },
   "outputs": [],
   "source": [
    "@dataclass\n",
    "class ReActStep:\n",
    "    \"\"\"\n",
    "    Data structure for a single ReAct step.\n",
    "    \n",
    "    This encapsulates one iteration of the thought-action-observation cycle.\n",
    "    Using a dataclass ensures consistent structure and makes the trace analyzable.\n",
    "    \"\"\"\n",
    "    iteration: int      # Which iteration is this?\n",
    "    thought: str        # What the model reasoned\n",
    "    action: Dict       # Structured action to take\n",
    "    observation: str    # Result from the environment\n",
    "    success: bool      # Did this step achieve the goal?\n",
    "    \n",
    "def parse_json_safely(text: str) -> Optional[Dict]:\n",
    "    \"\"\"\n",
    "    Safely parse JSON from model output.\n",
    "    \n",
    "    Language models often produce malformed JSON. This function attempts\n",
    "    to extract and fix common issues before parsing.\n",
    "    \n",
    "    Args:\n",
    "        text: Raw text that might contain JSON\n",
    "        \n",
    "    Returns:\n",
    "        Parsed dictionary or None if parsing fails\n",
    "    \"\"\"\n",
    "    # Strategy 1: Find the outer-most braces\n",
    "    # This handles cases where the model wraps JSON in markdown or adds extra text\n",
    "    start = text.find('{')\n",
    "    end = text.rfind('}')\n",
    "    \n",
    "    if start != -1 and end != -1 and end > start:\n",
    "        candidate = text[start:end+1]\n",
    "        try:\n",
    "            # strict=False allows control characters (like newlines) inside strings\n",
    "            return json.loads(candidate, strict=False)\n",
    "        except json.JSONDecodeError:\n",
    "            pass\n",
    "            \n",
    "    # Strategy 2: Fallback to the original regexes\n",
    "    json_patterns = [\n",
    "        r'\\{[^{}]*\\}',  # Simple JSON object\n",
    "        r'\\{.*\\}',       # Greedy JSON object  \n",
    "    ]\n",
    "    \n",
    "    for pattern in json_patterns:\n",
    "        matches = re.findall(pattern, text, re.DOTALL)\n",
    "        for match in matches:\n",
    "            try:\n",
    "                # Fix common issues\n",
    "                match = match.replace(\"'\", '\"')  # Single to double quotes\n",
    "                match = re.sub(r',\\s*}', '}', match)  # Remove trailing commas\n",
    "                match = re.sub(r',\\s*]', ']', match)  # Remove trailing commas in arrays\n",
    "                \n",
    "                # Attempt to parse\n",
    "                return json.loads(match, strict=False)\n",
    "            except json.JSONDecodeError:\n",
    "                continue\n",
    "            except Exception:\n",
    "                continue\n",
    "    \n",
    "    # If no valid JSON found, return None\n",
    "    return None"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4ef8aeeb",
   "metadata": {},
   "source": [
    "### Question 4 (Coding, 20 points): Implement Thought Generation\n",
    "\n",
    "**Objective**: Understand how to prompt language models to generate high-quality reasoning traces that guide action selection in agentic systems. Thought generation is critical because it determines what the agent tries next based on past failures.\n",
    "\n",
    "**Background**: In the ReAct pattern, thoughts serve as the reasoning layer. Good thoughts should:\n",
    "- Analyze the current situation (what's been tried, what failed)\n",
    "- Reference previous attempts and their outcomes\n",
    "- Propose a hypothesis about what to try next\n",
    "- Be specific and actionable (not vague like \"fix the bug\")\n",
    "\n",
    "**Your Task**: Implement the thought generation component of our ReAct system. This function prompts the model to reflect on the problem and its progress, generating a reasoning trace that will inform the next action.\n",
    "\n",
    "**Key Design Principle**: We deliberately separate thought generation (reasoning) from action generation (code production). This forces the model to plan before acting, which improves solution quality."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "79d50c02",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_thought(task: str, history: List[ReActStep], model: str = PRIMARY_MODEL, verbose: bool = False):\n",
    "    \"\"\"\n",
    "    Generate a reasoning step about what to do next.\n",
    "\n",
    "    TODO: Student Implementation (10-15 lines)\n",
    "    This function is the core coding task for **Question 4**.\n",
    "\n",
    "    Your thought generation should:\n",
    "    1. Create a prompt that includes the task and recent history\n",
    "    2. If there's history, analyze what went wrong previously\n",
    "    3. Generate a thought about what to try next\n",
    "    4. Optionally return the prompt for educational/debugging purposes\n",
    "\n",
    "    Implementation strategy:\n",
    "    - Start with the task description in your prompt\n",
    "    - If history exists, include the last 1-2 attempts with their observations\n",
    "    - Focus on what went wrong and why (e.g., \"failed because...\")\n",
    "    - Ask the model to reason about the next step, not generate code\n",
    "    - Use ollama.chat() to get the model's reasoning\n",
    "    - Return the thought as a string (or tuple if verbose=True)\n",
    "\n",
    "    Args:\n",
    "        task: The current task/problem\n",
    "        history: List of previous ReActStep objects\n",
    "        model: Model to use\n",
    "        verbose: If True, return (prompt, thought) tuple for debugging/education\n",
    "\n",
    "    Returns:\n",
    "        str: A thought/reasoning string (if verbose=False)\n",
    "        tuple: (prompt, thought) if verbose=True - useful for debugging and education\n",
    "\n",
    "    **Why verbose mode?** In production agentic systems, understanding the full\n",
    "    prompt-response cycle is crucial for:\n",
    "    - Debugging unexpected behaviors\n",
    "    - Learning prompt engineering through observation\n",
    "    - Understanding how context affects reasoning\n",
    "    - Iterating on prompt design\n",
    "\n",
    "    HINTS:\n",
    "    - Build a prompt string that includes context about the task\n",
    "    - Conditionally add information from recent history (if available)\n",
    "    - In your prompt, explicitly request reasoning/analysis, not code\n",
    "    - Use the ollama wrapper to call the model with your prompt\n",
    "    - Extract and return the text content from the model's response\n",
    "    - If verbose=True, return (prompt, thought) tuple instead of just thought\n",
    "\n",
    "    Example thought: \"The previous attempt failed because the search interval\n",
    "    wasn't updated correctly after checking mid. I should adjust how left and\n",
    "    right are updated in binary_search to avoid off-by-one errors.\"\n",
    "    \"\"\"\n",
    "\n",
    "    # TODO: Build the prompt\n",
    "    # Start with the task description to anchor the model's reasoning.\n",
    "    # This ensures all thoughts stay focused on the original goal.\n",
    "    prompt = ...\n",
    "    \n",
    "    # TODO: Add history context (if any)\n",
    "    # HINT: The history tells the model what it has already tried.\n",
    "    # 1. Check if 'history' is not empty.\n",
    "    # 2. Iterate through the last few steps (e.g., history[-2:]).\n",
    "    # 3. For each step, include the 'thought' and the 'observation'.\n",
    "    # \n",
    "    # EDUCATIONAL NOTE: We only include the last 2 attempts to:\n",
    "    # 1. Keep context window manageable for small models\n",
    "    # 2. Focus on recent failures (most relevant for next attempt)\n",
    "    # 3. Avoid overwhelming the model with too much history\n",
    "    if history:\n",
    "        prompt += ...\n",
    "        # Look at last 2 attempts to learn from recent failures\n",
    "        for step in history[-2:]:\n",
    "            # Truncate thought to 100 chars to keep prompt concise\n",
    "            prompt += ...\n",
    "            # Include full observation - this is the critical feedback signal\n",
    "            prompt += ...\n",
    "        prompt += ...\n",
    "    else:\n",
    "        prompt += ...\n",
    "\n",
    "    # TODO: Add instruction for thought generation\n",
    "    # CRITICAL: Explicitly tell the model NOT to write code yet.\n",
    "    # We want it to \"think\" (plan) before it \"acts\" (codes).\n",
    "    # This separation prevents the model from rushing into a bad solution.\n",
    "    # HINT: Explicitly tell the model to:\n",
    "    # 1. Analyze the situation.\n",
    "    # 2. Plan the next step.\n",
    "    # 3. NOT write code yet (this is crucial for ReAct).\n",
    "    #\n",
    "    # EDUCATIONAL NOTE: We explicitly tell the model NOT to write code yet.\n",
    "    # This separation of concerns (thinking vs. doing) is key to ReAct:\n",
    "    # - Thought: \"Why did X fail? I should try Y because...\"\n",
    "    # - Action: \"Here's the code implementing Y\"\n",
    "    prompt += ...\n",
    "\n",
    "    # TODO: Call the model and get the thought\n",
    "    # HINT: Use ollama.chat(model=model, messages=[...])\n",
    "    # The 'messages' list should contain a single dictionary: {'role': 'user', 'content': prompt}\n",
    "    response = ...\n",
    "    thought = ...\n",
    "\n",
    "    # TODO: Return based on verbose mode\n",
    "    # HINT:\n",
    "    # - If verbose is True, return a tuple: (prompt, thought)\n",
    "    # - If verbose is False, return just the string: thought\n",
    "    #\n",
    "    # EDUCATIONAL NOTE: The verbose mode is especially useful for:\n",
    "    # - Understanding why the model gave a particular response\n",
    "    # - Debugging prompt engineering issues\n",
    "    # - Teaching students about the prompt-response relationship\n",
    "    if verbose:\n",
    "        return prompt, thought\n",
    "    return thought"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a214d99e",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# =============================================================================\n",
    "# Test Suite for Question 4: Thought Generation\n",
    "# =============================================================================\n",
    "# These tests help you verify your implementation and understand the ReAct\n",
    "# reasoning process. We use verbose=True to show BOTH the prompt and response,\n",
    "# which is crucial for understanding prompt engineering and debugging.\n",
    "#\n",
    "# Key Learning Objectives:\n",
    "# 1. See how prompts are constructed with/without history\n",
    "# 2. Understand how model responses adapt to different contexts\n",
    "# 3. Observe the cause-and-effect relationship between prompt and output\n",
    "# 4. Learn to debug model behaviors by inspecting prompts\n",
    "\n",
    "print(\"=\" * 80)\n",
    "print(\"TEST 1: Generating thought WITHOUT history\")\n",
    "print(\"=\" * 80)\n",
    "print()\n",
    "print(\"📚 CONTEXT: This simulates the agent's first attempt at a task.\")\n",
    "print(\"   The model has no prior failures to learn from, so it will\")\n",
    "print(\"   provide a general analysis of the problem.\\n\")\n",
    "\n",
    "# TODO: Student - Call generate_thought with verbose=True\n",
    "# HINT: Use unpacking to get both prompt and thought: prompt, thought = generate_thought(...)\n",
    "# HINT: The verbose parameter should be set to True\n",
    "prompt_no_history, thought_no_history = generate_thought(\n",
    "    \"Fix the binary_search function\",\n",
    "    [],\n",
    "    verbose=True\n",
    ")\n",
    "\n",
    "print(\"📤 PROMPT SENT TO LLM:\")\n",
    "print(\"-\" * 80)\n",
    "print(prompt_no_history)\n",
    "print()\n",
    "\n",
    "print(\"📥 LLM RESPONSE:\")\n",
    "print(\"-\" * 80)\n",
    "print(thought_no_history)\n",
    "print()\n",
    "\n",
    "\n",
    "# =============================================================================\n",
    "# Test 2: With History\n",
    "# =============================================================================\n",
    "print(\"\\n\" + \"=\" * 80)\n",
    "print(\"TEST 2: Generating thought WITH history\")\n",
    "print(\"=\" * 80)\n",
    "print()\n",
    "print(\"📚 CONTEXT: This simulates an agent that has tried twice and failed.\")\n",
    "print(\"   Notice how the prompt now includes previous attempts and their\")\n",
    "print(\"   failures. The model should reflect on WHY they failed and propose\")\n",
    "print(\"   a better approach.\\n\")\n",
    "\n",
    "# Create fake history with two failed attempts\n",
    "# This simulates a realistic debugging scenario where:\n",
    "# 1. First attempt had a syntax error (easy to fix)\n",
    "# 2. Second attempt had logical errors (requires deeper reasoning)\n",
    "fake_history = [\n",
    "    ReActStep(\n",
    "        iteration=1,\n",
    "        thought=\"I tried adjusting the right boundary of the search interval.\",\n",
    "        action={\"tool\": \"fix_code\", \"params\": {\"code\": \"...\"}},\n",
    "        observation=\"Compilation error: syntax error on line 3\",\n",
    "        success=False,\n",
    "    ),\n",
    "    ReActStep(\n",
    "        iteration=2,\n",
    "        thought=\"I fixed the syntax but the tests are still failing for duplicate values.\",\n",
    "        action={\"tool\": \"fix_code\", \"params\": {\"code\": \"...\"}},\n",
    "        observation=(\n",
    "            \"Pass rate: 50%. Example failing test: \"\n",
    "            \"([1,2,2,2,3], 2) -> expected 1, got 2\"\n",
    "        ),\n",
    "        success=False,\n",
    "    ),\n",
    "]\n",
    "\n",
    "# TODO: Student - Call generate_thought with history and verbose=True\n",
    "# HINT: Pass fake_history as the second argument\n",
    "# HINT: Use verbose=True to see both prompt and response\n",
    "prompt_with_history, thought_with_history = generate_thought(\n",
    "    \"Fix the binary_search function\",\n",
    "    fake_history,\n",
    "    verbose=True\n",
    ")\n",
    "\n",
    "print(\"📤 PROMPT SENT TO LLM:\")\n",
    "print(\"-\" * 80)\n",
    "print(prompt_with_history)\n",
    "print()\n",
    "\n",
    "print(\"📥 LLM RESPONSE:\")\n",
    "print(\"-\" * 80)\n",
    "print(thought_with_history)\n",
    "print()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c977dd64",
   "metadata": {},
   "source": [
    "#### Self-Check: Thought Generation & Prompt Design (Optional)\n",
    "\n",
    "After reviewing the outputs above, you may briefly reflect on:\n",
    "\n",
    "- **Prompt vs. History**: How does adding recent attempts and outcomes change the model's\n",
    "  reasoning compared to the first attempt with no history?\n",
    "- **Reasoning Quality**: Does the second response use history to propose more concrete\n",
    "  next steps, or does it stay vague?\n",
    "- **Prompt Design**: What single change to the thought prompt (wording, structure, or\n",
    "  instructions) might most improve the model's reasoning for this task or a similar\n",
    "  real-world debugging setting?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0182df64",
   "metadata": {
    "lines_to_next_cell": 0
   },
   "source": [
    "### Question 5 (Coding, 20 points): Implement Action Generation\n",
    "\n",
    "**Objective**: Learn how to convert reasoning traces (thoughts) into structured, executable actions. This is where the ReAct pattern translates plans into concrete steps the agent can take.\n",
    "\n",
    "**Background**: After the model generates a thought about what to try next, we need to convert that reasoning into a structured action. In our code debugging domain, actions are JSON objects like:\n",
    "```json\n",
    "{\"tool\": \"fix_code\", \"params\": {\"code\": \"def binary_search(...)...\"}}\n",
    "```\n",
    "\n",
    "**Why JSON?** Structured output enables:\n",
    "- Reliable parsing (vs. free-form text)\n",
    "- Type safety (we know what fields to expect)\n",
    "- Tool routing (the \"tool\" field determines what happens next)\n",
    "- Composability (easy to add new tools later)\n",
    "\n",
    "**Your Task**: Implement action generation that prompts the model to produce valid JSON actions based on its reasoning. You'll also handle parsing failures gracefully - a critical skill when working with small models that sometimes produce malformed output.\n",
    "\n",
    "**Design Note**: The helpers you build here (generate_thought and generate_action) are intentionally generic: they should work for any debugging task described in the task string, not just the binary_search example. Later, we'll reuse the same components on the QuixBugs benchmark."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dec91b47",
   "metadata": {
    "lines_to_next_cell": 1
   },
   "outputs": [],
   "source": [
    "def generate_action(thought: str, task: str, model: str = PRIMARY_MODEL) -> Dict:\n",
    "    \"\"\"\n",
    "    Generate a structured action based on the thought.\n",
    "    \n",
    "    TODO: Student Implementation (10-12 lines)\n",
    "    This function is the core coding task for **Question 5**.\n",
    "    1. Create a prompt that converts thought into action\n",
    "    2. Request a specific JSON format\n",
    "    3. Parse and validate the response\n",
    "    \n",
    "    Implementation strategy:\n",
    "    - Create a prompt that includes the thought and task\n",
    "    - Request JSON in format: {\"tool\": \"fix_code\", \"params\": {\"code\": \"...\"}}\n",
    "    - Call ollama.chat() to get the model's response\n",
    "    - Use parse_json_safely() to extract JSON from the response\n",
    "    - Validate that 'tool' and 'params' keys exist\n",
    "    - If parsing fails or invalid, return error action\n",
    "    \n",
    "    Args:\n",
    "        thought: The reasoning about what to do\n",
    "        task: The original task\n",
    "        model: Model to use\n",
    "        \n",
    "    Returns:\n",
    "        Dictionary with 'tool' and 'params' keys\n",
    "        \n",
    "    HINTS:\n",
    "    - Create a prompt that provides the thought and task as context\n",
    "    - In your prompt, specify the exact JSON structure you want\n",
    "    - Call the model to generate the JSON action\n",
    "    - Use the provided parse_json_safely() helper to extract valid JSON\n",
    "    - Validate the parsed result has required keys before returning\n",
    "    - Return an error action dictionary if validation fails\n",
    "    \"\"\"\n",
    "\n",
    "    # TODO: Create prompt for action generation\n",
    "    # We force the model to output JSON so our code can reliably parse it.\n",
    "    # Natural language is hard for machines to read; JSON is easy.\n",
    "    # We include both the thought (reasoning) and task (context) to help the model\n",
    "    # generate appropriate code.\n",
    "    prompt = ...\n",
    "\n",
    "    # TODO: Call the model\n",
    "    # HINT: Use ollama.chat() again.\n",
    "    #\n",
    "    # EDUCATIONAL NOTE: Some models struggle with JSON output. Common issues:\n",
    "    # - Extra text before/after JSON\n",
    "    # - Single quotes instead of double quotes\n",
    "    # - Trailing commas\n",
    "    # - Missing closing braces\n",
    "    # This is why we need robust parsing in the next step!\n",
    "    response = ...\n",
    "    response_text = ...\n",
    "\n",
    "    # TODO: Parse the response\n",
    "    # HINT: Use the helper function parse_json_safely(response_text).\n",
    "    #\n",
    "    # EDUCATIONAL NOTE: parse_json_safely() handles common JSON errors:\n",
    "    # 1. Extracts JSON patterns with regex\n",
    "    # 2. Fixes single quotes → double quotes\n",
    "    # 3. Removes trailing commas\n",
    "    # This makes our system robust to small model output quality issues\n",
    "    parsed_action = ...\n",
    "\n",
    "    # TODO: Validate and return\n",
    "    # HINT:\n",
    "    # 1. Check if parsed_action is not None.\n",
    "    # 2. Check if it has the required keys: 'tool' and 'params'.\n",
    "    # 3. If valid, return the parsed_action.\n",
    "    # 4. If invalid, return a dictionary with tool=\"error\" and a message in params.\n",
    "    #\n",
    "    # EDUCATIONAL NOTE: Always validate structured output before using it!\n",
    "    # This prevents crashes when the model produces unexpected formats.\n",
    "    # Our error action {\"tool\": \"error\", ...} follows the same structure,\n",
    "    # so downstream code can handle it uniformly.\n",
    "    if ...:\n",
    "        return parsed_action\n",
    "    else:\n",
    "        return {\"tool\": \"error\", \"params\": {\"message\": \"Failed to parse valid action from model response\"}}\n",
    "\n",
    "# Test your implementation (students can use this as a self-check)\n",
    "test_action = generate_action(\n",
    "    \"The current function implementation is failing some tests and likely has a logic bug.\",\n",
    "    \"Fix the binary_search function\",\n",
    ")\n",
    "print(\"Generated action (basic structure check):\")\n",
    "print(json.dumps(test_action, indent=2))\n",
    "\n",
    "# Simple structural check: we expect a 'fix_code' tool with a 'code' field in params.\n",
    "if test_action.get('tool') != 'fix_code' or 'code' not in test_action.get('params', {}):\n",
    "    print(\n",
    "        \"Warning: expected tool='fix_code' with a 'code' field in 'params'.\\n\"\n",
    "        \"Check that your generate_action prompt requests the exact JSON format and that \"\n",
    "        \"parse_json_safely is used correctly.\"\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "64aa07cb",
   "metadata": {},
   "source": [
    "### Question 6 (Coding, 25 points): Complete the ReAct Loop\n",
    "\n",
    "**Objective**: Implement the complete ReAct control flow that orchestrates thought generation, action generation, action execution, and observation collection. This is where all the components come together into an agentic system.\n",
    "\n",
    "**Background**: The ReAct loop is the heart of an agentic system. It repeatedly:\n",
    "1. **Think**: Generate a thought based on task and history\n",
    "2. **Act**: Convert thought into a structured action\n",
    "3. **Execute**: Run the action (in our case, test code)\n",
    "4. **Observe**: Collect feedback from the environment\n",
    "5. **Decide**: Continue iterating or stop if successful\n",
    "\n",
    "This cycle enables the agent to learn from failures and adapt its approach.\n",
    "\n",
    "**Your Task**: Complete the main loop that coordinates these components. You'll implement the iteration logic, state management (tracking history), and termination conditions.\n",
    "\n",
    "**Key Design Decisions**:\n",
    "- **Max iterations**: Prevents infinite loops (agent might never solve hard problems)\n",
    "- **Success criteria**: What counts as \"done\"? (all tests passing in our case)\n",
    "- **History tracking**: Each iteration adds a ReActStep to enable learning\n",
    "- **Verbose mode**: Helps debug agent behavior by showing intermediate steps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "48f5665c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def react_loop(\n",
    "    task: str,\n",
    "    buggy_code: str,\n",
    "    test_cases: List[Tuple],\n",
    "    max_iterations: int = 5,\n",
    "    model: str = PRIMARY_MODEL,\n",
    "    verbose: bool = True\n",
    ") -> Tuple[str, List[ReActStep]]:\n",
    "    \"\"\"\n",
    "    Complete ReAct loop for code debugging.\n",
    "    \n",
    "    TODO: Student Implementation (20-30 lines)\n",
    "    This function is the core coding task for **Question 6**.\n",
    "    Complete the main loop that coordinates thought, action, and observation.\n",
    "    \n",
    "    Args:\n",
    "        task: Description of the task\n",
    "        buggy_code: The buggy code to fix\n",
    "        test_cases: Test cases to validate against\n",
    "        max_iterations: Maximum iterations before giving up\n",
    "        model: Model to use\n",
    "        verbose: Whether to print progress\n",
    "        \n",
    "    Returns:\n",
    "        Tuple of (final_code, history)\n",
    "        \n",
    "    IMPLEMENTATION STEPS:\n",
    "    1. Initialize history list and current_code\n",
    "    2. Loop for max_iterations\n",
    "    3. Generate thought using generate_thought()\n",
    "    4. Generate action using generate_action()\n",
    "    5. Execute action (if tool is \"fix_code\", extract code from params)\n",
    "    6. Create observation by testing the code\n",
    "    7. Check if all tests pass - if yes, break\n",
    "    8. Add step to history\n",
    "    9. Return final code and history\n",
    "    \"\"\"\n",
    "    \n",
    "    # Initialize state variables\n",
    "    # history: accumulates all ReActSteps for analysis and learning\n",
    "    # current_code: the candidate solution we're testing\n",
    "    # final_code: the best solution we've found (returned at the end)\n",
    "    history = []\n",
    "    current_code = buggy_code\n",
    "    final_code = buggy_code\n",
    "\n",
    "    for iteration in range(max_iterations):\n",
    "        if verbose:\n",
    "            print(f\"\\n--- Iteration {iteration + 1} ---\")\n",
    "\n",
    "        # TODO: Step 1 - Generate thought\n",
    "        # HINT: Call generate_thought(task, history, model)\n",
    "        #\n",
    "        # EDUCATIONAL NOTE: The thought generation sees the entire history,\n",
    "        # allowing it to learn from past failures. This is the key difference\n",
    "        # from single-shot approaches - the agent can reflect and adapt.\n",
    "        thought = ...\n",
    "\n",
    "        if verbose:\n",
    "            print(f\"Thought: {thought[:100]}...\")\n",
    "\n",
    "        # TODO: Step 2 - Generate action\n",
    "        # HINT: Call generate_action(thought, task, model)\n",
    "        #\n",
    "        # EDUCATIONAL NOTE: We pass both the thought (what to try) and\n",
    "        # the task (what we're solving) to ground the action generation.\n",
    "        action = ...\n",
    "\n",
    "        if verbose:\n",
    "            print(f\"Action: {action['tool']}\")\n",
    "\n",
    "        # TODO: Step 3 - Execute action\n",
    "        # HINT:\n",
    "        # 1. Check if action['tool'] is 'fix_code'.\n",
    "        # 2. If so, extract the code from action['params']['code'].\n",
    "        # 3. Update current_code and final_code with this new code.\n",
    "        #\n",
    "        # EDUCATIONAL NOTE: We validate the action before executing.\n",
    "        # If action parsing failed, action['tool'] will be 'error',\n",
    "        # and we'll skip code extraction (current_code stays unchanged).\n",
    "        # This defensive programming prevents crashes from malformed actions.\n",
    "        if action['tool'] == 'fix_code' and 'code' in action.get('params', {}):\n",
    "            current_code = ...\n",
    "            final_code = ...\n",
    "\n",
    "        # TODO: Step 4 - Test the code\n",
    "        # HINT: Call execute_code_safely(current_code, test_cases)\n",
    "        #\n",
    "        # EDUCATIONAL NOTE: We always test current_code, even if action\n",
    "        # extraction failed. This provides useful feedback (\"still broken\")\n",
    "        # rather than silently continuing with old code.\n",
    "        test_results = ...\n",
    "\n",
    "        # TODO: Step 5 - Create observation string\n",
    "        # HINT: Create a string that summarizes the test results.\n",
    "        # - If compilation failed, say \"Compilation error: ...\"\n",
    "        # - If tests failed, say \"Pass rate: X%. Failed: <example>\"\n",
    "        # - If all passed, say \"All tests passed!\"\n",
    "        #\n",
    "        # EDUCATIONAL NOTE: The observation is the agent's primary feedback signal.\n",
    "        # We format it to be maximally informative:\n",
    "        # - Compilation errors: exact error message (helps fix syntax)\n",
    "        # - Test failures: show one concrete failing case (helps fix logic)\n",
    "        # - Success: confirm all tests passed (enables termination)\n",
    "        if test_results['compilation'] != 'success':\n",
    "            observation = ...\n",
    "        else:\n",
    "            observation = ...\n",
    "\n",
    "        # TODO: Step 6 - Check success\n",
    "        # HINT: success = test_results['all_passed']\n",
    "        #\n",
    "        # EDUCATIONAL NOTE: Success means ALL tests passed, not just some.\n",
    "        # This is important for correctness - partial solutions aren't enough.\n",
    "        success = ...\n",
    "\n",
    "        # TODO: Step 7 - Create and add ReActStep to history\n",
    "        # HINT: Create a ReActStep object with all the current iteration data\n",
    "        # and append it to the history list.\n",
    "        #\n",
    "        # EDUCATIONAL NOTE: We record every step, even failures.\n",
    "        # This history enables:\n",
    "        # 1. Future iterations to learn from mistakes\n",
    "        # 2. Post-hoc analysis of agent behavior\n",
    "        # 3. Debugging when things go wrong\n",
    "        step = ...\n",
    "        history.append(step)\n",
    "\n",
    "        # TODO: Step 8 - Break if successful\n",
    "        # We stop immediately once we find a working solution (\"greedy\" approach).\n",
    "        # In a more advanced system, you might keep searching to find a *better* solution.\n",
    "        # HINT: If success is True, break the loop.\n",
    "        #\n",
    "        # EDUCATIONAL NOTE: We stop as soon as we solve the problem.\n",
    "        # This is the \"greedy\" termination strategy. Alternatives include:\n",
    "        # - Continue to find multiple solutions (for diversity)\n",
    "        # - Continue to improve solution quality (for optimization)\n",
    "        if success:\n",
    "            break\n",
    "\n",
    "        if verbose:\n",
    "            print(f\"Success: {success}\")\n",
    "\n",
    "    return final_code, history\n",
    "\n",
    "# Test your implementation\n",
    "print(\"Testing ReAct Loop Implementation\\n\")\n",
    "fixed_code, history = react_loop(\n",
    "    \"Fix the binary_search function\", \n",
    "    BUGGY_BINARY_SEARCH, \n",
    "    STRICT_TESTS,\n",
    "    max_iterations=5\n",
    ")\n",
    "\n",
    "print(f\"\\nFinal Results:\")\n",
    "print(f\"  Iterations used: {len(history)}\")\n",
    "print(f\"  Success: {history[-1].success if history else False}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "568d041e",
   "metadata": {},
   "source": [
    "### Required Analysis for Question 6\n",
    "\n",
    "After running the ReAct loop on the `binary_search` task (and optionally on a few QuixBugs problems), reflect on:\n",
    "- How often does ReAct succeed compared to the single-shot baseline (on the same tests)?\n",
    "- On average, how many iterations does ReAct need when it succeeds? What does this say about the cost of iteration?\n",
    "- When ReAct fails, which component seems most responsible (thought quality, action JSON validity, or tool feedback)?\n",
    "- Based on your traces, what concrete change would you try next to improve the agent?\n",
    "\n",
    "Summarize your observations in a brief written answer, making sure to tie them back to the earlier single-shot metrics and STRICT tests."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "11efd8f3",
   "metadata": {},
   "source": [
    "## Bonus Explorations: Advanced Topics in Agentic AI\n",
    "\n",
    "You've now built a working ReAct agent and evaluated it on real debugging tasks. The sections below offer four independent directions for deeper exploration. **Choose one** as a bonus project worth up to 20 additional points.\n",
    "\n",
    "Each option includes:\n",
    "- **Objective**: What you'll investigate\n",
    "- **Why it matters**: Real-world relevance\n",
    "- **Suggested approach**: Key ideas and code patterns (not a full implementation)\n",
    "- **Deliverables**: What to submit\n",
    "- **Evaluation criteria**: How your work will be assessed\n",
    "\n",
    "These are intentionally open-ended. We provide starting points, but you have freedom to explore directions that interest you."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0250293c",
   "metadata": {},
   "source": [
    "---\n",
    "### Bonus Option A: Component-Level Evaluation\n",
    "\n",
    "**Objective**\n",
    "\n",
    "When a ReAct loop fails, where did things go wrong? Was the reasoning poor? Did action parsing fail? Did the agent converge too slowly? End-to-end success rate tells you *if* your agent works, but component-level metrics tell you *why*.\n",
    "\n",
    "**Why it matters**\n",
    "\n",
    "Production agentic systems need targeted debugging. If your agent has 60% success rate, you need to know whether to improve the prompt (thought quality), the output parser (action validity), or the tool feedback (observation richness). Component metrics guide engineering effort toward the highest-impact improvements.\n",
    "\n",
    "**Suggested approach**\n",
    "\n",
    "Build an evaluation framework that scores each component independently:\n",
    "\n",
    "1. **Thought quality**: Does the reasoning reference previous failures? Is it specific and actionable? You can use keyword matching (e.g., presence of \"because\", \"failed\", \"try instead\") or ask a second LLM call to rate the thought on a 1-5 scale.\n",
    "\n",
    "2. **Action validity**: What percentage of generated actions parse as valid JSON with the expected schema? Track `tool` and `params` fields separately.\n",
    "\n",
    "3. **Convergence speed**: How many iterations until success (or max iterations)? Plot the distribution across multiple trials.\n",
    "\n",
    "**Deliverables**\n",
    "\n",
    "1. A brief report (1-2 pages) describing your evaluation methodology\n",
    "2. Results from running at least 10 trials, showing:\n",
    "   - Distribution of thought quality scores\n",
    "   - Action validity rate\n",
    "   - Correlation between component scores and overall success\n",
    "3. One visualization (e.g., box plot of component scores, or scatter plot of thought quality vs. success)\n",
    "4. A paragraph identifying which component is the bottleneck in your system and why\n",
    "\n",
    "**Evaluation criteria**\n",
    "\n",
    "- Soundness of evaluation methodology (do your metrics actually measure what you claim?)\n",
    "- Quality of analysis (do you draw meaningful conclusions from the data?)\n",
    "- Clarity of presentation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dd3249b4",
   "metadata": {},
   "source": [
    "---\n",
    "### Bonus Option B: Tool Design Principles\n",
    "\n",
    "**Objective**\n",
    "\n",
    "The quality of feedback from tools dramatically affects agent performance. A tool that returns \"Error\" gives the agent nothing to learn from. A tool that returns \"SyntaxError at line 5: unexpected indent\" enables targeted fixes. Investigate how tool design impacts ReAct effectiveness.\n",
    "\n",
    "**Why it matters**\n",
    "\n",
    "In production systems, you often control tool design but not model capabilities. Understanding what makes tools effective lets you maximize agent performance without changing the underlying LLM. This principle applies across domains: code execution, web search, database queries, API calls.\n",
    "\n",
    "**Suggested approach**\n",
    "\n",
    "Compare ReAct performance with tools that vary in feedback richness:\n",
    "\n",
    "1. **Minimal feedback**: Returns only \"Success\" or \"Error\"\n",
    "2. **Basic feedback**: Returns error type and pass/fail count\n",
    "3. **Rich feedback**: Returns error location, failing test cases with inputs/outputs, and hints\n",
    "\n",
    "Key code patterns to consider:\n",
    "```python\n",
    "def minimal_executor(code: str, tests: List) -> str:\n",
    "    \"\"\"Minimal feedback - just success/error.\"\"\"\n",
    "    try:\n",
    "        # ... execute code ...\n",
    "        return \"Success\" if all_passed else \"Error\"\n",
    "    except:\n",
    "        return \"Error\"\n",
    "\n",
    "def rich_executor(code: str, tests: List) -> str:\n",
    "    \"\"\"Rich feedback with diagnostic information.\"\"\"\n",
    "    try:\n",
    "        namespace = {}\n",
    "        exec(code, namespace)\n",
    "    except SyntaxError as e:\n",
    "        return f\"SyntaxError at line {e.lineno}: {e.msg}\\nProblematic code: {e.text}\"\n",
    "    except Exception as e:\n",
    "        return f\"RuntimeError: {type(e).__name__}: {e}\"\n",
    "    \n",
    "    # Run tests and show first failure in detail\n",
    "    for test_input, expected in tests:\n",
    "        actual = namespace['func'](test_input)\n",
    "        if actual != expected:\n",
    "            return f\"Test failed: input={test_input}, expected={expected}, got={actual}\"\n",
    "    return \"All tests passed!\"\n",
    "```\n",
    "\n",
    "Run experiments comparing success rate and iteration count across tool designs. Consider:\n",
    "- Does richer feedback reduce iterations needed?\n",
    "- Are there diminishing returns to feedback detail?\n",
    "- Does feedback style matter (structured vs. natural language)?\n",
    "\n",
    "**Deliverables**\n",
    "\n",
    "1. Implementation of at least two tool variants with different feedback richness\n",
    "2. Experimental results from at least 10 trials per tool variant, showing:\n",
    "   - Success rate comparison\n",
    "   - Average iterations to solution\n",
    "3. A table or chart comparing the tool variants\n",
    "4. A short discussion (0.5-1 page) extracting 3-5 principles for effective tool design that would generalize to other domains\n",
    "\n",
    "**Evaluation criteria**\n",
    "\n",
    "- Experimental rigor (controlled comparison, sufficient trials)\n",
    "- Insight quality (do your principles generalize beyond code debugging?)\n",
    "- Practical applicability of recommendations"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b126426b",
   "metadata": {},
   "source": [
    "---\n",
    "### Bonus Option C: Comparative Analysis of Small Language Models\n",
    "\n",
    "**Objective**\n",
    "\n",
    "Different models exhibit distinct behaviors in agentic contexts. Some excel at reasoning but struggle with structured output. Others reliably generate valid JSON but make logical errors. Systematically compare models to understand these trade-offs.\n",
    "\n",
    "**Why it matters**\n",
    "\n",
    "Model selection for agentic applications differs from model selection for chat or completion. A model that writes beautiful prose might fail at generating valid JSON actions. Understanding model-specific strengths helps you choose the right model for your use case and design appropriate scaffolding.\n",
    "\n",
    "**Suggested approach**\n",
    "\n",
    "Benchmark 2-3 models on multiple dimensions:\n",
    "\n",
    "1. **Task success rate**: What percentage of problems does each model solve?\n",
    "2. **Structural reliability**: What percentage of actions are valid JSON with correct schema?\n",
    "3. **Reasoning quality**: How specific and actionable are the generated thoughts?\n",
    "4. **Efficiency**: How many tokens/iterations does each model need?\n",
    "\n",
    "Key considerations for fair comparison:\n",
    "```python\n",
    "# Use identical prompts across models\n",
    "STANDARD_THOUGHT_PROMPT = \"\"\"...\"\"\"\n",
    "STANDARD_ACTION_PROMPT = \"\"\"...\"\"\"\n",
    "\n",
    "# Run multiple trials to account for variance\n",
    "N_TRIALS = 5\n",
    "\n",
    "# Track multiple metrics per trial\n",
    "def benchmark_model(model_name: str, n_trials: int) -> Dict:\n",
    "    results = []\n",
    "    for _ in range(n_trials):\n",
    "        fixed_code, history = react_loop(task, buggy_code, tests, model=model_name)\n",
    "        results.append({\n",
    "            'success': history[-1].success if history else False,\n",
    "            'iterations': len(history),\n",
    "            'valid_json_rate': sum(1 for s in history if s.action.get('tool') != 'error') / len(history),\n",
    "            # ... other metrics\n",
    "        })\n",
    "    return aggregate_results(results)\n",
    "```\n",
    "\n",
    "Consider visualizations like:\n",
    "- Bar charts comparing success rates\n",
    "- Radar/spider charts showing multi-dimensional performance\n",
    "- Heatmaps of model × metric scores\n",
    "\n",
    "**Deliverables**\n",
    "\n",
    "1. Benchmark results for at least 2 models across at least 3 metrics\n",
    "2. Visualizations comparing model performance\n",
    "3. A short analysis (0.5-1 page) addressing:\n",
    "   - Which model performs best overall? On which metrics?\n",
    "   - Are there trade-offs (e.g., better reasoning vs. better structured output)?\n",
    "   - What does this suggest about model selection for agentic applications?\n",
    "\n",
    "**Evaluation criteria**\n",
    "\n",
    "- Experimental design (fair comparison, controlled variables)\n",
    "- Depth of analysis (beyond just \"model X is better\")\n",
    "- Quality of visualizations"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "82ca3a32",
   "metadata": {},
   "source": [
    "---\n",
    "### Bonus Option D: Failure Mode Analysis and Recovery\n",
    "\n",
    "**Objective**\n",
    "\n",
    "Agentic systems fail in systematic, predictable ways. Identifying and categorizing these failure modes enables targeted improvements. Go beyond measuring success rate to understand *how* and *why* your agent fails.\n",
    "\n",
    "**Why it matters**\n",
    "\n",
    "In production, you can't just report \"40% failure rate.\" You need to know: Are failures due to parsing errors (fixable with better prompts)? Infinite loops (fixable with detection)? Fundamental reasoning limitations (requires model upgrade)? Failure analysis guides engineering priorities.\n",
    "\n",
    "**Suggested approach**\n",
    "\n",
    "Build a failure taxonomy and analyzer:\n",
    "\n",
    "1. **Failure categories**:\n",
    "   - `infinite_loop`: Agent repeats the same action 3+ times\n",
    "   - `invalid_json`: More than half of actions fail to parse\n",
    "   - `syntax_error`: Generated code has syntax errors\n",
    "   - `logic_error`: Code compiles but fails tests\n",
    "   - `gives_up`: Agent produces empty or non-code actions\n",
    "   - `timeout`: Exceeds max iterations without success\n",
    "\n",
    "2. **Recovery strategies**: Once you identify failure modes, implement targeted recovery:\n",
    "   - For loops: Inject a prompt saying \"You're repeating yourself. Try a completely different approach.\"\n",
    "   - For syntax errors: Add a \"focus on syntax first\" hint\n",
    "   - For logic errors: Include the failing test case in the next prompt\n",
    "\n",
    "**Deliverables**\n",
    "\n",
    "1. A failure taxonomy with at least 4 categories\n",
    "2. Results from analyzing at least 15 failed trials, showing:\n",
    "   - Distribution of failure modes (pie chart or bar chart)\n",
    "   - Examples of each failure type\n",
    "3. Implementation of at least one recovery strategy\n",
    "4. Before/after comparison showing whether recovery improves success rate\n",
    "5. A short discussion (0.5 page) on which failures are model-related vs. implementation-related\n",
    "\n",
    "**Evaluation criteria**\n",
    "\n",
    "- Completeness of failure taxonomy\n",
    "- Quality of failure analysis\n",
    "- Effectiveness of recovery strategy\n",
    "- Insight into root causes"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8617a1a3",
   "metadata": {},
   "source": [
    "---\n",
    "### Submission Guidelines for Bonus Projects\n",
    "\n",
    "**Format**: Submit a Jupyter notebook or PDF containing your analysis. Include:\n",
    "- Code cells showing your implementation (does not need to be runnable, but should be readable)\n",
    "- Results and visualizations\n",
    "- Written analysis addressing the deliverables\n",
    "\n",
    "**Length**: 3-5 pages total (including figures)\n",
    "\n",
    "**Grading**: Up to 20 bonus points based on:\n",
    "- Technical correctness (40%)\n",
    "- Depth of analysis (30%)\n",
    "- Clarity of presentation (20%)\n",
    "- Creativity and insight (10%)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "29e518c5",
   "metadata": {},
   "source": [
    "---\n",
    "## Final Reflection\n",
    "\n",
    "Whether or not you complete a bonus project, take a few minutes to reflect on what you've learned:\n",
    "\n",
    "**Synthesis questions** (no submission required, but worth thinking about):\n",
    "\n",
    "1. **Where does intelligence reside?** In the ReAct loop you built, is the intelligence in the model, the scaffolding (prompts, tools, loop structure), or the interaction between them?\n",
    "\n",
    "2. **Generalization**: How would you adapt the ReAct pattern to a completely different domain (medical diagnosis, financial analysis, creative writing)? What would thoughts, actions, and observations look like?\n",
    "\n",
    "3. **Trade-offs**: Iteration improves performance but adds latency and cost. How would you determine the right balance for a specific application?\n",
    "\n",
    "4. **Safety**: Agentic systems can take real-world actions. What safeguards would you implement for a production agent that executes code or modifies files?\n",
    "\n",
    "The techniques you've learned form the foundation for building robust agentic systems. As you apply these patterns to new domains, remember: the goal isn't perfection on the first try, but systems that recognize and correct their own mistakes."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0b163d58",
   "metadata": {
    "lines_to_next_cell": 0
   },
   "source": [
    "## Conclusions\n",
    "\n",
    "In this notebook, you:\n",
    "- Built a **single-shot debugging baseline** and saw how weak specs and tests can dramatically overestimate model capability.\n",
    "- Strengthened the specification with STRICT tests and a real benchmark (QuixBugs), revealing more realistic single-shot limits.\n",
    "- Implemented a **ReAct-style agent** that iterates through thoughts, actions, and observations instead of committing to a single guess.\n",
    "- Explored how evaluation choices (pass@K, average pass rate, component-level metrics) shape your understanding of agent performance.\n",
    "\n",
    "Taken together, these pieces illustrate that much of the \"intelligence\" in agentic systems comes from the **scaffolding** you design: prompts, tools, tests, and control loops, not just from the base model.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c8d4300b",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "## References and Acknowledgments\n",
    "\n",
    "- **ReAct framework**: Yao et al., 2022, \"ReAct: Synergizing Reasoning and Acting in Language Models\".\n",
    "- **QuixBugs benchmark**: Lin et al., a collection of classic algorithmic bugs and tests used here as a small, realistic debugging suite.\n",
    "- **Machine Learning: Learn by Building**: This notebook is part of the broader course materials developed by Prof. Ming Jin.\n",
    "\n",
    "Large language models and tools were used as assistants in drafting portions of this notebook. All code, prompts, and analysis prompts were curated and reviewed to align with the educational goals of the course."
   ]
  }
 ],
 "metadata": {
  "jupytext": {
   "cell_metadata_filter": "-all",
   "formats": "ipynb,py:percent"
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
